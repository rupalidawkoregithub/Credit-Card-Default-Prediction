{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rupalidawkoregithub/Credit-Card-Default-Prediction/blob/main/Individual_Credit_Card_Default_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Credit Card Default Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rupali Dawkore\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project aimed at predicting the case of customers default payment in taiwan. data is of 29999 customers with 6 months transiction history.\n",
        "\n",
        "###**Evaluation Criteria:**\n",
        "The evaluation criteria was based on Recall value , F1 SCore and Accurance percentages. Higher the score better will be the prediction\n",
        "\n",
        "###**Problem Statement:**\n",
        "The main objective of this project - credit card default prediction is to develop a model that accurately predicts the likelihood of a credit card holder defaulting on their debt payment in the near future. The model should take into account various factors such as the cardholder's credit history and other relevant financial information. The goal is to minimize false negatives (predicting a non-default when the cardholder actually defaults) and false positives (predicting a default when the cardholder is able to repay their debt) while maximizing the overall accuracy of the prediction.\n",
        "\n",
        "###**Approach:**\n",
        "The project for credit card default prediction aims to develop a model that can accurately predict the likelihood of a credit card holder defaulting on their payments. The project typically includes the following steps:\n",
        "\n",
        "**Data cleaning and preprocessing:** Removing missing or irrelevant data and transforming the data into a format suitable for analysis.\n",
        "\n",
        "**Exploratory Data Analysis (EDA):** Analyzing the distribution of the data and identifying any patterns or trends that may be relevant to the prediction model.different types of graphs by separating them into univariate, bivariate and multivariate categories as a result, We came accross some meaningful insights that helped us to make future decisions of ML model pipeline\n",
        "\n",
        "**Feature selection:** Choosing the most relevant variables to include in the prediction model based on the results of the EDA.Under the umbrella of feature engineering we have detected and treated the outliers with the help of IQR technique and capped all the outliers of continous features in 25-75 percentile. Also, we noticed that some of the features were categorical in nature and ML model can not understand the language of alphabets(strings).So, we have encoded them into numericals using One-Hot Encoding technique as they were unordered in nature.\n",
        "\n",
        "**Model development:** Training and testing different machine learning algorithms, such as decision trees, random forests, and XG Boot, to find the best model for predicting credit card default.\n",
        "\n",
        "**Model evaluation:** Evaluating the performance of the selected model using metrics such as accuracy, precision, recall, and F1 score.\n",
        "\n",
        "The successful implementation of this model can help credit card companies make informed decisions about the risk associated with lending to individual customers and can also aid customers in managing their credit card debt."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem statement for credit card default prediction is to develop a model that accurately predicts the likelihood of a credit card holder defaulting on their debt payment in the near future. The model should take into account various factors such as the cardholder's credit history and other relevant financial information. The goal is to minimize false negatives (predicting a non-default when the cardholder actually defaults) and false positives (predicting a default when the cardholder is able to repay their debt) while maximizing the overall accuracy of the prediction."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing basic libraries for data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from datetime import datetime\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# Adding this to ignore future warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# importing missingo library which helps us to visualize the missing values\n",
        "import missingno as msno\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler "
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade xlrd         ##run the cell at every restart of runtime"
      ],
      "metadata": {
        "id": "S4HZTCNMbKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the Google Drive for Import the Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x6PuQc7VZXgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/Credit Card Default Prediction - Classification/default of credit card clients.xls',header=1)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked first five rows of dataset"
      ],
      "metadata": {
        "id": "MXK4Hkp9JKpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "_sRe1yLgpKOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked last five rows of dataset"
      ],
      "metadata": {
        "id": "WhKmXv-3JWQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape"
      ],
      "metadata": {
        "id": "Zw9fBat1D_LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of rows and columns\n",
        "print(\"Number of rows: \", rows)\n",
        "print(\"Number of columns: \", columns)"
      ],
      "metadata": {
        "id": "3k1RWu_hED2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 30000 rows and 25 columns in the dataset."
      ],
      "metadata": {
        "id": "HcFoK03V2R3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All types of columns dtypes are integer"
      ],
      "metadata": {
        "id": "JckJsvI2EOfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicated rows in default of credit card clients dataset: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not have any duplicated rows in the dataset and that is very good for us."
      ],
      "metadata": {
        "id": "5BV1D1R6GRN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values \n",
        "print(f\"Null values count in default of credit card clients dataset:\\n{df.isna().sum()}\\n\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Infinite values count in default of credit card clients dataset:\\n{df.isin([np.inf, -np.inf]).sum()}\\n\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't have null or infinite values default of credit card clients dataset."
      ],
      "metadata": {
        "id": "AH3sol0WG1ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values in default of credit card clients dataset\n",
        "msno.bar(df,figsize=(10,5), color=\"tab:green\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset dose not contains any NA values, null values and duplicates."
      ],
      "metadata": {
        "id": "h3Cp1u3N3FPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data provided is a sample of a credit card default dataset. The first row provides the header information, with each column indicating a feature of the credit card holder. The first column (ID) is the unique identifier for each record. The second column (LIMIT_BAL) indicates the credit limit of the credit card. The third column (SEX) indicates the gender of the cardholder. The fourth column (EDUCATION) indicates the level of education of the cardholder. The fifth column (MARRIAGE) indicates the marital status of the cardholder. The sixth column (AGE) indicates the age of the cardholder. The seventh to sixteenth columns (PAY_0 to PAY_9) indicate the repayment status for the last ten months. The remaining columns (BILL_AMT1 to BILL_AMT6, PAY_AMT1 to PAY_AMT6) indicate the amount of bill statement and amount paid in the last six months. The final column (default payment next month) is the target variable and indicates whether the cardholder defaulted on their payment in the next month (1) or not (0)."
      ],
      "metadata": {
        "id": "IFexwD-0IE_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Descriptive Statistics"
      ],
      "metadata": {
        "id": "S5PnxaCLMi9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we see the descriptive statistics such as count, mean, standard deviation, minimum,maximum, quantiles"
      ],
      "metadata": {
        "id": "HXcnnCaMp6ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "0_yQ9-7-MIMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "- There are around 30000 distict credit card clients.\n",
        "- The average value of credit card Limits is Rs 1,67,484.\n",
        "- The Limited Balance has a high Standard deviation as the meadian value is Rs 1,40,000 and the extreme values as Rs 10,00,000.\n",
        "- Here the average is about 35 and meadian is 28 with a standard deviation of 9.2. This difference is explained by some very old people in the data set as given that the maximum age is 79.\n",
        "- Bill Amount and Pay Amount also shows us that there some people with extremely high bill amount which may be because for the higher Credit Limit or because of the pending dues added up. \n",
        "- Bill amount for all the months, the mean is around 40,000 to 50,000 with some extreme amount in bill amount 3 of Rs 16,64,089.\n",
        "- Pay amount for all the months, the mean is around Rs 4800 to Rs 5800, with some extreme values such as Rs 16,64,089.\n",
        "- As the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis)."
      ],
      "metadata": {
        "id": "QIUShNzQMzTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ID:** ID of each client\n",
        "\n",
        "**LIMIT_BAL:** Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "\n",
        "**SEX:** Gender (1=male, 2=female)\n",
        "\n",
        "**EDUCATION:** (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "\n",
        "**MARRIAGE:** Marital status (1=married, 2=single, 3=others)\n",
        "\n",
        "**AGE:** Age in years\n",
        "\n",
        "**PAY_0:** Repayment status in September, 2005 (-1=pay duly, 1=payment delay for\n",
        "\n",
        "one month, 2=payment delay for two months,8=payment delay for eight months,\n",
        "\n",
        "9=payment delay for nine months and above)\n",
        "\n",
        "**PAY_2:** Repayment status in August, 2005 (scale same as above)\n",
        "\n",
        "**PAY_3:** Repayment status in July, 2005 (scale same as above)\n",
        "\n",
        "**PAY_4:** Repayment status in June, 2005 (scale same as above)\n",
        "\n",
        "**PAY_5:** Repayment status in May, 2005 (scale same as above)\n",
        "\n",
        "**PAY_6:** Repayment status in April, 2005 (scale same as above)\n",
        "\n",
        "**BILL_AMT1:** Amount of bill statement in September, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT2:** Amount of bill statement in August, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT3:** Amount of bill statement in July, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT4:** Amount of bill statement in June, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT5:** Amount of bill statement in May, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT6:** Amount of bill statement in April, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT1:** Amount of previous payment in September, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT2:** Amount of previous payment in August, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT3:** Amount of previous payment in July, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT4:** Amount of previous payment in June, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT5:** Amount of previous payment in May, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT6:** Amount of previous payment in April, 2005 (NT dollar)\n",
        "\n",
        "**default.payment.next.month:** Default payment (1=yes, 0=no)"
      ],
      "metadata": {
        "id": "IA2xmjFPC0-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "8EEhURuJOZCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "def get_all_unique_values(df):\n",
        "    for col in df.columns:\n",
        "        print(f\"Unique values in column '{col}':\")\n",
        "        print(df[col].unique())\n",
        "        print(\"-\"*50)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and print all unique values\n",
        "get_all_unique_values(df)"
      ],
      "metadata": {
        "id": "dxNjz2e-OkI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We dont need to ID Column so we drop it\n",
        "df.drop(['ID'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "pHb1C_GNx5xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 25\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "KC3tdDdJfDc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "AkCaOzCjHgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copied dataset for data wrangling."
      ],
      "metadata": {
        "id": "9pwN0ofircYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "rN9dnBr4qYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the columns didnâ€™t make sense to me, so I decided to rename them into more understandable terms."
      ],
      "metadata": {
        "id": "HJscieUqPXqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename target column\n",
        "df1 = df.rename(columns={'default payment next month':'default'})"
      ],
      "metadata": {
        "id": "gJgm5bTyw4l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "df1.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "df1.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ],
      "metadata": {
        "id": "qT-NmuO_31fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing values with there labels\n",
        "df1.replace({'SEX': {1 : 'Male', 2 : 'Female'}}, inplace=True)\n",
        "df1.replace({'EDUCATION' : {1 : 'Graduate School', 2 : 'University', 3 : 'High School', 4 : 'Others'}}, inplace=True)\n",
        "df1.replace({'MARRIAGE' : {1 : 'Married', 2 : 'Single', 3 : 'Others'}}, inplace = True)\n",
        "df1.replace({'default' : {1 : 'Yes', 0 : 'No'}}, inplace = True)"
      ],
      "metadata": {
        "id": "8kELTtNPqOnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After renamed look of dataset\n",
        "df1.head(10)"
      ],
      "metadata": {
        "id": "Eo6gnaWuczht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df1['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "2Z9nUqL3vTiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In education column, values such as 5,6 and 0 are unknown. Let's combine those values as others."
      ],
      "metadata": {
        "id": "vnfBwXBNvpk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replcae values with 5, 6 and 0 to Others\n",
        "df1.EDUCATION = df1.EDUCATION.replace({5: \"Others\", 6: \"Others\",0: \"Others\"})"
      ],
      "metadata": {
        "id": "hYi4t4jkvzGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking value count of education column after combine others values\n",
        "df1['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "_wkOtdzev6ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df1['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "IzHawr0AzbZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In marriage column, 0 values are not known. Combine those values in others category."
      ],
      "metadata": {
        "id": "KeznT5KHzfFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replace 0 with Others\n",
        "df1.MARRIAGE = df1.MARRIAGE.replace({0: \"Others\"})"
      ],
      "metadata": {
        "id": "5VgPP9eQzjz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking\n",
        "df1['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "M2Fpju3jzx89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Distribution of default or non-default case**\n",
        "\n"
      ],
      "metadata": {
        "id": "vm1_Wc7CFOOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi0 = df[df['default payment next month']==0]\n",
        "mi1 = df[df['default payment next month']==1]"
      ],
      "metadata": {
        "id": "v8gaQHwvDaR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi0.value_counts().sum()  # Non-Default => 0 or NO"
      ],
      "metadata": {
        "id": "5Pt0d8Tr6935"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi1.value_counts().sum()  # Default => 1 or YES"
      ],
      "metadata": {
        "id": "nGWkw9g87TXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Dbm8Ozgi5nnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_col=['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n",
        "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
        "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
        "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "# Checking Distribution of Non - Default \n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(con_col):\n",
        "    plt.subplot(6,5,num+1)\n",
        "    sns.distplot(mi0[i],color='g')\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-3dgG4bZEibT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Distribution of Default Case \n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(con_col):\n",
        "    plt.subplot(6,5,num+1)\n",
        "    sns.distplot(mi1[i],color='r')\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TMXxsGfR8rj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "wBU2X5xmQfma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. I have Changed the Name of a columns with its meaningfull name\n",
        "\n",
        "2. Grouped unknown EDUCATIONcategories (0,5,6) and re-assigned them to 4 (others)\n",
        "\n",
        "3. Grouped unknown MARRIAGEcategories (0) and re-assigned them to 3 (others)\n",
        "\n",
        "4. Checking Distribution of default or non-default case"
      ],
      "metadata": {
        "id": "a1mm8yqSQd__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate**"
      ],
      "metadata": {
        "id": "3ITwYZhn2xfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "RBp5zjWutAU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 1"
      ],
      "metadata": {
        "id": "irXXTAQ9XXtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting payment staus using countplot\n",
        "pay_col = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR']\n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(pay_col):\n",
        "    plt.subplot(4,3,num+1)\n",
        "    sns.countplot(df1[i])\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "47BbyFnw7x7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each column represents the status of payment for a given month:\n",
        "\n",
        "**1.'PAY_SEPT':** Represents the payment status in September.\n",
        "\n",
        "**2.'PAY_AUG':** Represents the payment status in August.\n",
        "\n",
        "**3.'PAY_JUL':** Represents the payment status in July.\n",
        "\n",
        "**4.'PAY_JUN':** Represents the payment status in June.\n",
        "\n",
        "**5.'PAY_MAY':** Represents the payment status in May.\n",
        "\n",
        "**6.'PAY_APR':** Represents the payment status in April.\n",
        "\n",
        "The countplot is used to visualize the distribution of the values in each of these columns. It shows the count of each unique value in the column, creating a histogram-like representation. The goal of this visualization is to see the frequency of each payment status, which can help understand the overall payment behavior of credit card customers in the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cnpUeZe4SHqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 2"
      ],
      "metadata": {
        "id": "Ieq9ExSuXfIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of balance limit of credit card of customer**"
      ],
      "metadata": {
        "id": "5wpAXVI8TYl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig1 = px.histogram(df1, x = 'LIMIT_BAL', marginal = 'box',\n",
        "                    title = 'Distribution of balance limit of card', \n",
        "                    labels = {'x': 'Dollar($)', 'y': 'Number of card'},\n",
        "                   color_discrete_sequence=px.colors.qualitative.Antique)\n",
        "fig1.update_layout(width=900, height=700)\n",
        "fig1.show()"
      ],
      "metadata": {
        "id": "XETY9KJGOk2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is found that limit balance feature is **right skewed, middle 50% of value lie between 50K to 240k. few of limit goes beyond 530k taiwan dollar.**"
      ],
      "metadata": {
        "id": "qTFQKTN96BKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ZqjUzsL8Yd8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Histogram chart was picked because it is well-suited for visualizing the distribution of a single variable and provides useful information about the **shape and properties of the data**."
      ],
      "metadata": {
        "id": "-flDDX3uYejf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "64jfLvyaY7tx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of using this specific chart is to visualize the distribution of the balance limit of credit cards. The histogram provides a visual representation of the frequency of each balance limit range, while the box plot summarizes the distribution by showing the median, quartiles, and outliers."
      ],
      "metadata": {
        "id": "oj1eS8EeY-oh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? "
      ],
      "metadata": {
        "id": "peoZwIBtY_jL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, Insights can help the business to better understand their customers and make informed decisions about credit risk."
      ],
      "metadata": {
        "id": "5_6ZFnYLZAOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Distribution of Target column on the basis of Percentage**"
      ],
      "metadata": {
        "id": "uY7bzcgIqKU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Target column on the basis of Percentage\n",
        "plt.figure(figsize = [10, 5])\n",
        "plt.title(\"Default payment next month\")\n",
        "df1[\"default\"].value_counts().plot.pie(explode = [0, 0.10], autopct = '%1.3f%%', shadow = True)"
      ],
      "metadata": {
        "id": "0zBKHlrlb3Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "r34AoUog97MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are typically used to compare the values of categorical data and show the distribution of data over time. They are good for visualizing data for a small number of categories and allow for easy comparison of values."
      ],
      "metadata": {
        "id": "tA-hSfPE97MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "XlLWYNiZ97MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that Number of not default cardholders have higher % then Defaulters"
      ],
      "metadata": {
        "id": "NEpbfqh997MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "hvDcEpMy97MW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the positive impact that less number of cardholders are defaulters"
      ],
      "metadata": {
        "id": "eBKh87tk97MW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# values count plot of Default\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'default', data = df1)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot gives us an insight into the class distribution in the target variable. It is observed that the classes are not proportionate, indicating an imbalanced dataset. The data shows that there are **23,000 non-defaulters and 6,000 defaulters** , which means that this is a case of **imbalanced data**."
      ],
      "metadata": {
        "id": "4ToPhUdDwSvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 3"
      ],
      "metadata": {
        "id": "e03a4pAFXnHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# values count plot of Marriage\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'MARRIAGE', data = df1)"
      ],
      "metadata": {
        "id": "C6I0OP9Gz-uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bar plot was chosen because it is a straightforward and effective way to visualize the distribution of categorical data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bivariate**"
      ],
      "metadata": {
        "id": "dg6v7vC4C7fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 4"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**What is a qualification of the card holder**"
      ],
      "metadata": {
        "id": "dNIk-rF5cQ6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vis = df1['EDUCATION'].value_counts().reset_index()\n",
        "df_vis.columns = ['Education', 'No of people']\n",
        "fig1 = px.pie(df_vis, values = 'No of people', names = 'Education',color_discrete_sequence =  px.colors.sequential.Plasma,\n",
        "             title = 'Education qualification of credit card holder')\n",
        "fig1.update_layout(width=500, height=400)\n",
        "fig1.show()"
      ],
      "metadata": {
        "id": "acVZm9LrbEy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here,\n",
        "2. -->  **University**\n",
        "1. -->  **Graduate School**\n",
        "3. -->  **High School**\n",
        "4. -->  **Others** \n",
        "\n",
        "*   47% of them have university qualification\n",
        "*   35% of them have graduate school qualification \n",
        "*   16% of them have high school qualification \n",
        "*   2% of them qualification are unknown"
      ],
      "metadata": {
        "id": "tLtCDT1WcjO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "cituyOWl6m1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie Chart : It is suitable for visualizing the distribution of categorical data, such as education levels in this case. Pie charts are effective in showing how each category contributes to the total and are easy to understand for most people. The use of a pie chart allows for a quick and clear representation of the distribution of education levels among credit card holders in the dataset."
      ],
      "metadata": {
        "id": "9tXcuc2j6ni2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "M5vzSpvs6oCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights found from the chart can be determined by looking at the proportions of the different education levels in the pie chart. Example If the majority of the credit card holders have a certain education level, this may suggest a trend or pattern in terms of who is more likely to have a credit card."
      ],
      "metadata": {
        "id": "dYOrzbms6o0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?"
      ],
      "metadata": {
        "id": "QZmk3Hzd6pTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the goal of the business is to target certain segments of the population with marketing campaigns, the information from this insight could be used to identify education levels that are overrepresented or underrepresented among credit card holders."
      ],
      "metadata": {
        "id": "upmlvtur6p4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 5"
      ],
      "metadata": {
        "id": "BGlm3QTC_L6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of default customers whose age is below 40 or Above 40**"
      ],
      "metadata": {
        "id": "m8Xim16u1rN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# violin plot using the Seaborn library that visualizes the distribution of ages of the credit card holders who are less than 40 years old\n",
        "sns.violinplot(x='default', y='AGE', data=df1[df1['AGE']<40], palette='cool')"
      ],
      "metadata": {
        "id": "GZusKtkk1qP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "violinplot : That visualizes the distribution of ages of credit card holders who are less than 40 years old, split by their payment default status."
      ],
      "metadata": {
        "id": "LYVn_Xet331_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ayM3-Ysq4Qeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot specifically used to compare the distribution of ages for credit card holders who defaulted on their payments and those who did not. The violin plot allows us to visualize the distribution of ages for each category and identify any potential differences or similarities in the age distribution between the two categories. This information can be useful in understanding the relationship between age and payment default status."
      ],
      "metadata": {
        "id": "1LUMswBA4lQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2Oeh3s3h45FK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights that can be found from this violin plot are based on the comparison of the distributions of the ages of credit card holders who defaulted on their payments and those who did not.\n",
        "Some potential insights that could be gained from this chart include:\n",
        "    \n",
        "    \n",
        "   \n",
        "    \n",
        "\n",
        "*   **Distribution Shape :** The shape of the violin plot can provide information about the distribution of the data.\n",
        "\n",
        "*   **Central Tendency:** The box inside the violin plot shows the median and quartiles of the data, which give an indication of the central tendency of the distribution.\n",
        "*    **Spread:** The width of the violin plot represents the density of the data, giving an indication of the spread of the distribution.\n",
        "\n",
        "\n",
        "*   **Outliers:** The violin plot can also show any outliers in the data, which are values that fall outside of the distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "p4X3sgkB45r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bar plot to visualize the relationship between the age of the credit card holder and the default status who are greater than 40 years old\n",
        "pd.crosstab(df1[df1['AGE']>40]['AGE'],df1[df1['default']==1]['default']).plot(kind='bar')"
      ],
      "metadata": {
        "id": "hlms3LmG4mbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot : The data is filtered to only include credit card holders whose age is greater than 40, and the default status is filtered to only include those who defaulted on their payments. The plot shows a cross-tabulation of the age and default status, with the number of occurrences of each combination of age and default status represented by the height of the bar. "
      ],
      "metadata": {
        "id": "8UTjKhbh3qYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 6"
      ],
      "metadata": {
        "id": "YjZFrlcaYHlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of Age of credit card holder**"
      ],
      "metadata": {
        "id": "VE7zhSO_ewU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = px.histogram(df1, x = 'AGE', marginal = 'box',\n",
        "                    title = 'Distribution of Age of card holder', \n",
        "                    labels = {'x': 'Dollar($)', 'y': 'Noumber of card'},\n",
        "                   color_discrete_sequence=px.colors.qualitative.D3,\n",
        "                   nbins = 75)\n",
        "fig2.update_layout(width=500, height=400)\n",
        "fig2.update_traces(marker_line_width=1,marker_line_color=\"white\")"
      ],
      "metadata": {
        "id": "WU8qiyN9c5g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ages of the credit card holders are taken from the**\"AGE\"**column in the DataFrame.The histogram has a **box plot** on the right side that shows the **distribution of the ages**.\n"
      ],
      "metadata": {
        "id": "zkGW83l6wN8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "OTnVXw9zwOel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram is used to visualize the distribution of numerical data.It can provide an understanding of the distribution of ages among the credit card holders in the dataset."
      ],
      "metadata": {
        "id": "L7JUzGYcwO_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "QyEZlLGqwQAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights found the distribution of ages in the histogram.It's shows the frequency of credit card holders within different age intervals, which can give us a general understanding of the age distribution among credit card holders. **Example**, If the histogram shows a concentration of credit card holders in a certain age range, this may suggest that people within that age range are more likely to have a credit card."
      ],
      "metadata": {
        "id": "_PoFClZHwQvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 7"
      ],
      "metadata": {
        "id": "wrgG6mz3YM0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Distribution of credit limit for default and non-default cases with limit balance**"
      ],
      "metadata": {
        "id": "iYkzFXWPxE4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"default\", y=\"LIMIT_BAL\", data=df1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAOmTqkfvsEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Sex With Respective to Default** \n",
        "\n",
        "**or** \n",
        "\n",
        "###**How many male and female are credit card defaulter.**"
      ],
      "metadata": {
        "id": "L4_gY-gnuWY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count plot for Sex and with respect to Default\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(10,5))\n",
        "sns.countplot(x = 'SEX', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'SEX', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "2CXJUzo6uB6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Education With Respective to Default**"
      ],
      "metadata": {
        "id": "4WwqcyhAy5ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count plot for EDUCATION and with respect to Default\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(18,5))\n",
        "sns.countplot(x = 'EDUCATION', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'EDUCATION', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "fyhz1Y_zxaYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Marriage With Respective to Default**"
      ],
      "metadata": {
        "id": "xxXhCvlF0XKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count plot for MARRIAGE and with respect to IsDefaulter\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(10,5))\n",
        "sns.countplot(x = 'MARRIAGE', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'MARRIAGE', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "1IoQSuvB0V6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate**"
      ],
      "metadata": {
        "id": "OlZxmcf4B0OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 8"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Creating new variables\n",
        "var = df1[['SEX', 'LIMIT_BAL','AGE']].copy()\n",
        "var['default'] = df1['default']\n",
        "\n",
        "#replace values in varibles with original names\n",
        "var.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}},inplace = True)\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking catplot for the given variable\n",
        "sns.catplot(x = \"SEX\",\n",
        "            y = \"LIMIT_BAL\",\n",
        "            kind = \"box\",\n",
        "            hue = \"default\",\n",
        "            color = '#0c4f4e',\n",
        "            data = var, saturation = 2,\n",
        "            margin_titles = True).set(title = \"limit balance by sex and default payments\");"
      ],
      "metadata": {
        "id": "EhQYGVFCZxx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catplot is used in Seaborn to create categorical plots, which are plots that show the relationship between a categorical variable i.e SEX and one continuous variables i.e LIMIT_BAL. These plots are useful for visualizing the distribution and spread of data."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "There are **more Female defaulters than men** ,female have more ouliers in Limit Balance Variable"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 9 \n",
        "\n",
        "### **Correlation Heatmap**"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "H5qsID3VgPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "XDE5hyWjJnNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing LabelEncoder library\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Convert categorical columns to numerical values using label encoding\n",
        "df1['SEX'] = le.fit_transform(df1['SEX'])\n",
        "df1['EDUCATION'] = le.fit_transform(df1['EDUCATION'])\n",
        "df1['MARRIAGE'] = le.fit_transform(df1['MARRIAGE'])\n",
        "df1['default'] = le.fit_transform(df1['default'])\n",
        "\n",
        "# Convert the columns to int data type\n",
        "df_copy = df1.astype({\"SEX\":\"int\",\"EDUCATION\":\"int\",\"MARRIAGE\":\"int\",\"default\":\"int\"})"
      ],
      "metadata": {
        "id": "CcN5fQ1BJwsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "sqD_6I6qNW7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize = [25, 15])\n",
        "corr_matrix = df_copy.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is use to find the corelation between different pararmentrs in the dataset."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the correlation matrix there no any clear picture but we can say that Pay and Bill amt are positively correlated and Pay negatively correlated with Pay amt."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Null Hypothesis - There is no relation between Categorical Variables and Default\n",
        "\n",
        "   Alternate Hypothesis - There is a relationship between Categorical Variables and Default\n",
        "\n",
        "2. Null Hypothesis - There is no relation between Numeric Variable and Default\n",
        "\n",
        "   Alternate Hypothesis - There is a relation between Numeric Variable and Default"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- Null Hypothesis - There is no relation between Categorical Variables and Default\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between Categorical Variables and Default"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.columns"
      ],
      "metadata": {
        "id": "dQy5WiM2hTRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def hypothesis_test_chi2(df_copy, categorical_variable, alpha=0.05):\n",
        "    # Split the data into default and non-default groups\n",
        "    default = df_copy[df_copy['default'] == 1]\n",
        "    non_default = df_copy[df_copy['default'] == 0]\n",
        "\n",
        "    # Conduct a chi-square test for the independence of the categorical variable and default\n",
        "    cont = pd.crosstab(df_copy['default'], df_copy[categorical_variable])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(cont)\n",
        "\n",
        "    # Make a decision based on the p-value and alpha\n",
        "    if p_value < alpha:\n",
        "        return f\"Reject the null hypothesis. There is a significant association between {categorical_variable} and default.\"\n",
        "    else:\n",
        "        return f\"Fail to reject the null hypothesis. There is no significant association between {categorical_variable} and default.\""
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of categorical variables\n",
        "categorical = ['SEX', 'EDUCATION', 'MARRIAGE','PAY_SEPT',\n",
        "       'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\n",
        "# Loop through the list of categorical variables\n",
        "for categorical_variable in categorical:\n",
        "    result = hypothesis_test_chi2(df_copy, categorical_variable)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "6yMoSKluczCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function hypothesis_test_chi2 performs a chi-square test for independence to obtain the p-value. The chi2_contingency function from the scipy.stats library is used to calculate the chi-square statistic and the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chi-square test for independence is a common test for determining if there is a significant association between two categorical variables. In this case, the categorical variable of interest and the binary outcome variable \"default\" are being tested for independence. The choice of the chi-square test for independence is appropriate for this type of analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Null Hypothesis - There is no relation between Numeric Variable and Default\n",
        "\n",
        "Alternate Hypothesis - There is a relation between Numeric Variable and Default"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def hypothesis_test_t(df_copy, numerical_variable, alpha=0.05):\n",
        "    # Split the data into default and non-default groups\n",
        "    default = df_copy[df_copy['default'] == 1][numerical_variable]\n",
        "    non_default = df_copy[df_copy['default'] == 0][numerical_variable]\n",
        "\n",
        "    # Conduct a two-sample t-test for the means of the numerical variable for default and non-default groups\n",
        "    t, p_value = ttest_ind(default, non_default)\n",
        "\n",
        "    # Make a decision based on the p-value and alpha\n",
        "    if p_value < alpha:\n",
        "        return f\"Reject the null hypothesis. There is a significant difference in the means of {numerical_variable} between default and non-default groups.\"\n",
        "    else:\n",
        "        return f\"Fail to reject the null hypothesis. There is no significant difference in the means of {numerical_variable} between default and non-default groups.\""
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['LIMIT_BAL','AGE','BILL_AMT_SEPT',\n",
        "       'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "       'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL',\n",
        "       'PAY_AMT_JUN', 'PAY_AMT_MAY', 'PAY_AMT_APR']\n",
        "for col in numerical_columns:\n",
        "    result = hypothesis_test_t(df_copy, col, alpha=0.05)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "usSw4V8ddQ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function hypothesis_test_t performs a two-sample t-test to obtain the p-value. The ttest_ind function from the scipy.stats library is used to calculate the t-statistic and the p-value"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test is a common test for determining if there is a significant difference in means between two groups. In this case, the two groups are the default and non-default groups for a numerical variable. The choice of the two-sample t-test is appropriate for this type of analysis when the numerical variable is continuous and the sample size is relatively small."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To identify the categorical, numerical columns, and input and target columns"
      ],
      "metadata": {
        "id": "Ykqtm8fJoUb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.columns"
      ],
      "metadata": {
        "id": "q65VtquKusBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# independant variable\n",
        "Input_columns=[ 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_SEPT',\n",
        "       'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR', 'BILL_AMT_SEPT',\n",
        "       'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "       'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL',\n",
        "       'PAY_AMT_JUN', 'PAY_AMT_MAY', 'PAY_AMT_APR']\n",
        "# dependent variable\n",
        "Target_column=[\"default\"]"
      ],
      "metadata": {
        "id": "E8hv2sVflkVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = ['SEX','EDUCATION','MARRIAGE','PAY_SEPT', 'PAY_AUG',\n",
        "       'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']"
      ],
      "metadata": {
        "id": "QcG-5RMWv0B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['LIMIT_BAL','AGE','BILL_AMT_SEPT',\n",
        "       'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "       'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL',\n",
        "       'PAY_AMT_JUN', 'PAY_AMT_MAY', 'PAY_AMT_APR']"
      ],
      "metadata": {
        "id": "5reteQ9fCCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "rQp32GBNkf9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for null value by using .info\n",
        "df_copy.info()"
      ],
      "metadata": {
        "id": "y2kP_7mfkVDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking skewness for numerical columns\n",
        "import scipy.stats as stats\n",
        "for col in numerical_columns:\n",
        "    skewness = stats.skew(df_copy[col])\n",
        "    print(\"Skewness of column {}: {:.2f}\".format(col, skewness))"
      ],
      "metadata": {
        "id": "CRMmstBAGX-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for num,cols in enumerate(numerical_columns):\n",
        "    plt.subplot(5,3,num+1)\n",
        "    sns.boxplot(df_copy[cols])\n",
        "    plt.title(f'{cols.title()}',weight='bold')\n",
        "    plt.tight_layout()\n",
        "    #print(' Box Plot of',cols)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UuCI52wyDRo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the boxplot, it can be observed that the columns: LIMIT_BAL , AGE ,  BILL_AMT_SEPT , BILL_AMT_AUG , BILL_AMT_JUL , BILL_AMT_JUN , BILL_AMT_MAY , BILL_AMT_APR , PAY_AMT_SEPT , PAY_AMT_AUG , PAY_AMT_JUL , PAY_AMT_JUN , PAY_AMT_MAY , and  PAY_AMT_APR contain outliers.\n"
      ],
      "metadata": {
        "id": "AdoZ4EyGFB7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Outlier Records:\")\n",
        "\n",
        "for col in numerical_columns:\n",
        "    upper = df_copy[col].quantile(0.75) + 1.5 * (df_copy[col].quantile(0.75) - df_copy[col].quantile(0.25))\n",
        "    outliers = df_copy[df_copy[col] > upper][col].count()\n",
        "    print(\"{}: {}\".format(col, outliers))\n"
      ],
      "metadata": {
        "id": "SpMaCYfZIYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the upper bound of outliers using the interquartile range (IQR) and then counts the number of values in each column that are greater than this upper bound."
      ],
      "metadata": {
        "id": "FV5PCr_HKa11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capping"
      ],
      "metadata": {
        "id": "CVH9KsLiKLjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_columns:\n",
        "    upper = df_copy[col].quantile(0.75) + 1.5 * (df_copy[col].quantile(0.75) - df_copy[col].quantile(0.25))\n",
        "    df_copy[col] = np.where(df_copy[col] > upper, upper, df_copy[col])\n",
        "    print(\"{}: {}\".format(col, outliers))\n"
      ],
      "metadata": {
        "id": "0tBnGRp7Jf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The upper bound of outliers is calculated using the interquartile range (IQR) as in the previous code. Then, using np.where, the values in each column that are greater than the upper bound are replaced with the upper bound. And performs the capping of the outliers."
      ],
      "metadata": {
        "id": "pBxGs61fKjYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking outliers for numerical columns\n",
        "plt.figure(figsize=(15,15))\n",
        "for num,cols in enumerate(numerical_columns):\n",
        "    plt.subplot(5,3,num+1)\n",
        "    sns.boxplot(df_copy[cols])\n",
        "    plt.title(f'{cols.title()}',weight='bold')\n",
        "    plt.tight_layout()\n",
        "    #print(' Box Plot of',cols)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jeeqnOWaKBeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "3O8m71yuiuB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "b0tU1oUoQhNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "def replace_values(df_copy, col, values):\n",
        "    # This code is replacing all values of -2, -1, and 0 in the specified columns with 0.\n",
        "    fil = (df_copy[col] == -2) | (df_copy[col] == -1) | (df_copy[col] == 0)\n",
        "    df_copy.loc[fil, col] = values\n",
        "\n",
        "columns = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\t\t\t\t\n",
        "values = 0\n",
        "\n",
        "for col in columns:\n",
        "    replace_values(df_copy, col, values)"
      ],
      "metadata": {
        "id": "eb54Qr7qzotf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is replacing all values of -2, -1, and 0 in the specified columns with 0."
      ],
      "metadata": {
        "id": "bUCFBPhunRmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_values(df_copy, col, values):\n",
        "    #  This code is replacing all values greater then 0 in the specified columns with 0.\n",
        "    fil = (df_copy[col] < 0)\n",
        "    df_copy.loc[fil, col] = values\n",
        "\n",
        "columns = ['BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR']\t\t\t\t\t\t\n",
        "values = 0\n",
        "\n",
        "for col in columns:\n",
        "    replace_values(df_copy, col, values)"
      ],
      "metadata": {
        "id": "DIC53I2czu33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As negative Bill Amount paid indicates that the person has paid his due payment already. Hence, we transform the data as above"
      ],
      "metadata": {
        "id": "M9DuJZL3nvOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "fil = (df_copy.PAY_SEPT == 0) & (df_copy.PAY_AUG == 0) & (df_copy.PAY_JUL == 0) & (df_copy.PAY_JUN == 0) & (df_copy.PAY_MAY == 0) & (df_copy.PAY_APR == 0) & (df_copy['default'] == 1)\n",
        "df_copy.loc[fil,'default'] = 0\n"
      ],
      "metadata": {
        "id": "Tv9i8Ke9zyzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As customers who have not defaulted in any month cannot be potential defaulters hence, we have transformed the data as above."
      ],
      "metadata": {
        "id": "ngs9P5sBoPGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "fil = (df_copy.PAY_SEPT > 0) & (df_copy.PAY_AUG > 0) & (df_copy.PAY_JUL > 0) & (df_copy.PAY_JUN > 0) & (df_copy.PAY_MAY > 0) & (df_copy.PAY_APR > 0) & (df_copy['default'] == 0)\n",
        "df_copy.loc[fil,'default'] = 1"
      ],
      "metadata": {
        "id": "b83Af-zqz2wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As customers who have defaulted in every month are the potential defaulters hence, we have transformed the data as above."
      ],
      "metadata": {
        "id": "SIW4N34WoZiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "ZYruea35FbiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binning the 'AGE' column**"
      ],
      "metadata": {
        "id": "KgEt62cfwDOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_copy['AGE'].min())\n",
        "print(df_copy['AGE'].max())"
      ],
      "metadata": {
        "id": "685bpuFjuoWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see here min age is 21.0 and maximum age is 60.5 in our dataset"
      ],
      "metadata": {
        "id": "t2Szng5vr315"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to create the cohort for age group\n",
        "def age(x):\n",
        "    if x in range(21,41):\n",
        "        return 1\n",
        "    elif x in range(41,61):\n",
        "        return 2\n",
        "    elif x in range(61,80):\n",
        "        return 3\n",
        "\n",
        "df_copy['AGE']=df_copy['AGE'].apply(age)"
      ],
      "metadata": {
        "id": "6bx2d3HevP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "kCLxkFJ7Rm64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.isnull().sum()"
      ],
      "metadata": {
        "id": "nzWolLq_T8vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing the Null values with suitable value\n",
        "df_copy['AGE'].fillna(df_copy['AGE'].mode()[0],inplace=True)"
      ],
      "metadata": {
        "id": "mpHqXgZHUaU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the ages are into three categories, By applying the categorize_age function to the AGE column of the dataframe, the original ages in the column are transformed into categorical values."
      ],
      "metadata": {
        "id": "6olGOX3xsmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing age group\n",
        "plt.figure(figsize=(10,8),dpi=60)\n",
        "sns.countplot(x=df_copy['AGE'].sort_values(),data=df_copy,hue='default')"
      ],
      "metadata": {
        "id": "cVlKqw3Ys3fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "r9WyYbtJ0A4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**obsevation:**\n",
        "In aur dataset we can clearly see that most of the credit card holder are of age between 21 to 41 , so we can say that company's target customer are mostly youngster."
      ],
      "metadata": {
        "id": "A2SBiAIFtEjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binning the 'PAY' column**"
      ],
      "metadata": {
        "id": "nX_f50l4lSlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bins(x):\n",
        "    if x == -2:\n",
        "        return 'Paid Duly'\n",
        "    if x == 0:\n",
        "        return 'Paid Duly'\n",
        "    if x == -1:\n",
        "        return 'Paid Duly'\n",
        "    if x in range(1,4):\n",
        "        return '1 to 3'\n",
        "    if x in range(4,7):\n",
        "        return '4 to 6'\n",
        "    if x in range(7,9):\n",
        "        return '7 to 9'\n",
        "\n",
        "for i in df_copy[['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']]:\t\t\t\t\t\t\n",
        "    df_copy[i]=df_copy[i].apply(bins)"
      ],
      "metadata": {
        "id": "BuAUa4jWjYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values of the columns 'PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', and 'PAY_APR' in the DataFrame 'df_copy'.\n",
        "\n",
        "For each value of these columns, the 'bins' function is being applied, which maps the values to one of four categorical bins:\n",
        "\n",
        "Paid Duly (for values of -2, 0, or -1)\n",
        "\n",
        "1 to 3 (for values in the range 1 to 3)\n",
        "\n",
        "4 to 6 (for values in the range 4 to 6)\n",
        "\n",
        "7 to 9 (for values in the range 7 to 9)"
      ],
      "metadata": {
        "id": "E7Qu8Ux6ssjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.head()"
      ],
      "metadata": {
        "id": "U_KGE_hdje_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding**"
      ],
      "metadata": {
        "id": "eDBVI_xvWGF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#label encoding\n",
        "#encoders_nums = {\"SEX\":{\"Male\":0,\"Female\":1}, \"default\":{\"Yes\":1, \"No\":0}}\n",
        "#df1 = df1.replace(encoders_nums)"
      ],
      "metadata": {
        "id": "LDz2gJVuWEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for changed labels\n",
        "df_copy.head(5)"
      ],
      "metadata": {
        "id": "_-PBJj01WtBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "8fr9JHJXW4po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "Z_zLB5zJVbHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Importing\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical features\n",
        "categorical_cols_to_encode = ['EDUCATION', 'MARRIAGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']"
      ],
      "metadata": {
        "id": "UUMrxpSUUxA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(df_copy[categorical_cols_to_encode])\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols_to_encode))\n",
        "df_copy[encoded_cols] = encoder.transform(df_copy[categorical_cols_to_encode])"
      ],
      "metadata": {
        "id": "VyALwAq3U0Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_cols"
      ],
      "metadata": {
        "id": "LmrnFav7U3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.drop(['EDUCATION', 'MARRIAGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Jry5caNMU6-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.shape"
      ],
      "metadata": {
        "id": "NhKaCBcmU-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 47\n",
        "df_copy.head()"
      ],
      "metadata": {
        "id": "Ky4dZJ6zk0RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.columns"
      ],
      "metadata": {
        "id": "uS5AyIxV-yaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "EcrnNEXJV4ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy = df_copy.astype(int)"
      ],
      "metadata": {
        "id": "DrvhPrAPWejs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.info()"
      ],
      "metadata": {
        "id": "zlD8pk2HWhji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variable (estimator)\n",
        "X = df_copy.drop(\"default\", axis = 1)\n",
        "\n",
        "# dependent variable (label)\n",
        "y = df_copy[\"default\"]"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,shuffle=True, stratify=y, random_state = 42)"
      ],
      "metadata": {
        "id": "vXWCRT4XeV_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X_train_scale = scaler.fit_transform(X_train)\n",
        "X_test_scale = scaler.transform(X_test)\n",
        "\n",
        "X_train = pd.DataFrame(X_train_scale, columns = X_train.columns)\n",
        "X_test = pd.DataFrame(X_test_scale, columns = X_test.columns)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Checking if Data is Imbalance"
      ],
      "metadata": {
        "id": "YyAiJYzUwYMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((df_copy['default'].value_counts()/df_copy['default'].shape)*100)\n",
        "sns.countplot(df_copy['default'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIv2vxXKwe5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.default.value_counts() "
      ],
      "metadata": {
        "id": "7_bJM98Vz3-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, Here we can see that the data is imbalanced. bcoz the Based on the values 77.88% for 0 and 22.12% for 1), it appears that the dataset is imbalanced. An imbalanced dataset refers to a situation where the distribution of classes (in this case, 0s and 1s) is not equal. In the given code, the class distribution is heavily skewed towards class 0, with 77.88% of the observations being class 0 and only 22.12% being class 1. This imbalance can lead to difficulties in training machine learning models, as the model may be biased towards predicting the majority class. As a result, the model's performance on the minority class may be poor."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing SMOTE to handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()"
      ],
      "metadata": {
        "id": "v2kI1n6m32Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_smote, y_train_smote = smote.fit_resample(X,y)\n",
        "from collections import Counter\n",
        "print(\"Before SMOTE :\" , Counter(y_train))\n",
        "print(\"After SMOTE :\" , Counter(y_train_smote))"
      ],
      "metadata": {
        "id": "BTtEZsjr49A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Original dataset shape', len(df_copy))\n",
        "print('Resampled dataset shape', len(y_train_smote))"
      ],
      "metadata": {
        "id": "JMiJf0J-0Jpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(df_copy.columns)\n",
        "columns"
      ],
      "metadata": {
        "id": "-nQidmkryvfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping orignal columns\n",
        "df_copy.drop(['default'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "098NIxs1JNU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new Dataframe with balanced data\n",
        "balanced_df = pd.DataFrame(X_train_smote, columns=columns)"
      ],
      "metadata": {
        "id": "ma2oOSCzQcf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df['default'] = y_train_smote"
      ],
      "metadata": {
        "id": "1bJ2CYFKRUNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check shape of new daatframe\n",
        "balanced_df.shape"
      ],
      "metadata": {
        "id": "-4g8jhTnReRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting balanced data using countplot\n",
        "ax = sns.countplot('default', data = balanced_df)\n",
        "ax.set_xticklabels(('Not Defaulted','Defaulted'),rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PBJWMAf-RisG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variable (estimator)\n",
        "X = balanced_df.drop(\"default\", axis = 1)\n",
        "\n",
        "# dependent variable (label)\n",
        "y = balanced_df[\"default\"]"
      ],
      "metadata": {
        "id": "UmFg4yCyZ7AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle=True, stratify=y, random_state = 42)"
      ],
      "metadata": {
        "id": "aLCb9FWGaBc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique used to handle the imbalanced dataset is Synthetic Minority Over-sampling Technique (**SMOTE**). SMOTE is a commonly used oversampling technique for imbalanced datasets, which creates synthetic samples of the minority class instead of simply duplicating existing samples. This helps to balance the class distribution and reduce overfitting, which can occur when a model is trained on a highly imbalanced dataset. The reason for choosing SMOTE in this case is because it is effective in handling class imbalance by generating new samples of the minority class, while still preserving the characteristics of the original data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all needed libraries\n",
        "# Get the confusion matrix for both train and test\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, plot_precision_recall_curve\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "# Importing for model implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "# Importing for hyperparamerter tunning\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "f7k2UwD08gUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating Function**"
      ],
      "metadata": {
        "id": "ixuWvBE97yGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_accuracy=[]\n",
        "model_precision=[]\n",
        "model_recall=[]\n",
        "model_f1_score=[]\n",
        "model_roc_auc_score=[]"
      ],
      "metadata": {
        "id": "8kmv1ipdblJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Function to run diferent models\n",
        "def run_and_evaluate_model(model,X_train,X_test,y_train,y_test, best_parameter=True, best_score=True):\n",
        "  '''\n",
        "  train the model and gives mse,rmse,r2,adj r2 score of the model\n",
        "  can be used for any model where y is not transformed \n",
        "  '''\n",
        "  # Start trainning time\n",
        "  start=time.time()\n",
        "  #training the model\n",
        "  model.fit(X_train,y_train)\n",
        "  # Stop trainning time\n",
        "  stop = time.time()\n",
        "  time_min=round((stop - start)/60,4)\n",
        "  print(f\"Training time: {time_min}min\",'\\n')\n",
        "\n",
        "  #predicting the values of y from x via model\n",
        "  y_pred_test = model.predict(X_test)\n",
        "  y_pred_train = model.predict(X_train)\n",
        "\n",
        "  def score (model,X,actual,predicted,append=True):\n",
        "    accuracy = accuracy_score(actual,predicted)\n",
        "    precision = precision_score(actual,predicted)\n",
        "    recall = recall_score(actual,predicted)\n",
        "    f1 = f1_score(actual,predicted)\n",
        "    roc= roc_auc_score(actual,predicted)\n",
        "    confusion_mat=confusion_matrix(actual,predicted)\n",
        "    print(\"The accuracy is \", accuracy)\n",
        "    print(\"The precision is \", precision)\n",
        "    print(\"The recall is \", recall)\n",
        "    print(\"The f1 is \", f1)\n",
        "    print('the auc  is ',roc)\n",
        "    print('\\nconfusion_matrix \\n ',confusion_mat)\n",
        "    \n",
        "    if append==True:\n",
        "      model_accuracy.append(accuracy)\n",
        "      model_precision.append(precision)\n",
        "      model_recall.append(recall)\n",
        "      model_f1_score.append(f1)\n",
        "      model_roc_auc_score.append(roc)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  print('score matrix for train')\n",
        "  print('-'*40)\n",
        "  score(model=model,X=X_train,actual=y_train,predicted=y_pred_train,append=False)\n",
        "  print('\\nClassification Report\\n')\n",
        "  print(classification_report(y_train, y_pred_train))\n",
        "  print('\\n')\n",
        "  print('score matrix for test')\n",
        "  print('-'*40)\n",
        "  score(model=model,X=X_test,actual=y_test,predicted=y_pred_test)\n",
        "  print('\\nClassification Report\\n')\n",
        "  print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "W_fsYiwgbmr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**creating function to get feature importances**\n"
      ],
      "metadata": {
        "id": "oen646_eXUMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function to get features importance of all the tree based model\n",
        "def get_features_importance(optimal_model,X_train):\n",
        "  imp_feat=pd.DataFrame(index=X.columns,data=optimal_model.feature_importances_,columns=['importance'])\n",
        "  imp_feat=imp_feat[imp_feat['importance']>0]\n",
        "  imp_feat=imp_feat.sort_values('importance')\n",
        "  plt.figure(figsize=(15,5))\n",
        "  print(f'---------------------Features Importance------------------------\\n\\n {optimal_model}\\\n",
        "  \\n-------------------------------------------------------------------\\n') \n",
        "  sns.barplot(data=imp_feat,x=imp_feat.index,y='importance')\n",
        "  plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "CPpTNgRuMHuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression**"
      ],
      "metadata": {
        "id": "wff_dQO1_Msv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Instance of Logistic Regression\n",
        "LR= LogisticRegression()\n",
        "run_and_evaluate_model(LR, X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "eGbEWgLzNVjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an evaluation report for a logistic regression model. The model has an accuracy of 64% both on the training set and the test set. Precision is defined as the number of true positive predictions over the number of true positive and false positive predictions, and it is also 69% on both training and test sets. Recall is defined as the number of true positive predictions over the number of true positive and false negative predictions and it is 50% on the training set and 49% on the test set. The F1 score is the harmonic mean of precision and recall, and it is 58% on both the training and test sets. The AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve is a measure of model performance, and it is also 64% on both the training and test sets."
      ],
      "metadata": {
        "id": "JZzcnJSswrsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing GridSearchCV for hyperparameter tuning in Logistics Regreesion**"
      ],
      "metadata": {
        "id": "Y_G8RFjQW7WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating param dict for hyperparameter tuning\n",
        "param_dict= {'C': [0.001,0.01,0.1,1,10,100],'penalty': ['l1', 'l2'],'max_iter':[1000]} "
      ],
      "metadata": {
        "id": "_HttsZOmSV-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Grid model to perform grid search\n",
        "grid_log_model = GridSearchCV(LR, param_dict,n_jobs=-1, cv=5, verbose = 5,scoring='recall') "
      ],
      "metadata": {
        "id": "jd3jS7ZgSiZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#running and evaluating grid_log_model using function ccreated\n",
        "run_and_evaluate_model(grid_log_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "oxMSuV6PSsUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Features Importance Of Logistics Regression**"
      ],
      "metadata": {
        "id": "nKQZlHIzUotQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_log_model=grid_log_model.best_estimator_.coef_"
      ],
      "metadata": {
        "id": "FDv-rUBqTeB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dataframe for feature imp\n",
        "feature_importance = pd.DataFrame({'Features':X.columns, 'Importance':np.abs(optimal_log_model).ravel() })"
      ],
      "metadata": {
        "id": "MrgfMszAT6rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting the feat impt df by importance\n",
        "imp_feat = feature_importance.sort_values(by = 'Importance', ascending=False)[:10]"
      ],
      "metadata": {
        "id": "oNv54MSDUMvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imp_feat"
      ],
      "metadata": {
        "id": "JdYFV7HvUPs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the feature importance\n",
        "imp_feat=imp_feat[imp_feat['Importance']>0]\n",
        "imp_feat=imp_feat.sort_values('Importance')\n",
        "plt.figure(figsize=(15,5))\n",
        "print(f'---------------------Features Importance------------------------\\n\\n {optimal_log_model}\\\n",
        "\\n---------------------------Plot-----------------------------\\n') \n",
        "sns.barplot(data=imp_feat,x=imp_feat.Features,y='Importance')\n",
        "plt.xticks(rotation=90);"
      ],
      "metadata": {
        "id": "5CGHKPRjTqbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**"
      ],
      "metadata": {
        "id": "Yu1JJBcApTjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Instance of Random Forest\n",
        "Rf = RandomForestClassifier(n_estimators=50,random_state=12)\n",
        "run_and_evaluate_model(Rf, X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "hZaNWNTZEFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model has an accuracy of 0.97 on the training set and 0.94 on the test set, which means that it correctly classifies 97% and 94% of the samples in the training and test set, respectively. The precision of the model is 0.97 and 0.92 on the training and test set, respectively. Precision measures the proportion of true positive predictions among all positive predictions made by the model. The recall of the model is 0.98 and 0.95 on the training and test set, respectively. Recall measures the proportion of true positive predictions among all actual positive samples. The F1 Score is the harmonic mean of precision and recall, and it is 0.97 and 0.94 on the training and test set, respectively. Finally, the AUC is the area under the ROC curve and it measures the model's ability to distinguish between positive and negative samples. The AUC is 0.97 and 0.94 on the training and test set, respectively."
      ],
      "metadata": {
        "id": "ECDAhJdZzCDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing RandomizedSearchCV for hyperparameter tuning in Random Forest**"
      ],
      "metadata": {
        "id": "5uGwoy4Ft-xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating parameter grid  \n",
        "param_grid = {\n",
        "    'max_depth': [10,20,30],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [5, 8, 10],\n",
        "    'n_estimators': [100, 150, 200]\n",
        "}"
      ],
      "metadata": {
        "id": "N6enFEb1sodD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate grid search model\n",
        "random_search = RandomizedSearchCV(estimator = Rf,param_distributions= param_grid,  scoring = 'accuracy',  \n",
        "                                   cv = 3, n_jobs = -1, verbose = 1)"
      ],
      "metadata": {
        "id": "guzP2q8WssMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit grid search to the data\n",
        "random_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "25JGMa-8j9n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get best parameters\n",
        "random_search.best_params_"
      ],
      "metadata": {
        "id": "iF1MzFOukTd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get best score\n",
        "random_search.best_score_"
      ],
      "metadata": {
        "id": "Z_AMjU_1kf6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_and_evaluate_model(random_search, X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "DBRJJMuqklCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Features Importance Of Random Forest Classifier**"
      ],
      "metadata": {
        "id": "A_1OstGWWU_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#storing best estimator in varibale\n",
        "optimal__rfc_model=random_search.best_estimator_"
      ],
      "metadata": {
        "id": "H8G2sElLWXRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the feature importance of variable for random forest classifier\n",
        "get_features_importance(optimal__rfc_model,X_train)"
      ],
      "metadata": {
        "id": "QERODx3OWbDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **XG Boost**"
      ],
      "metadata": {
        "id": "Nz2odMAqotR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting data into XG Boosting Classifier\n",
        "xgb = XGBClassifier()\n",
        "run_and_evaluate_model(xgb, X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "07mUsBSLjjhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of the model on the training set is 0.92 and on the test set is 0.92. This means that the model correctly predicts 92% of the classes in both training and test sets.\n",
        "\n",
        "The precision of the model on the training set is 0.89 and on the test set is 0.89. Precision measures the proportion of true positive predictions among the positive predictions. In this case, 89% of the positive predictions made by the model are correct.\n",
        "\n",
        "The recall of the model on the training set is 0.96 and on the test set is 0.96. Recall measures the proportion of true positive predictions among all actual positive cases. In this case, the model correctly identifies 96% of the actual positive cases.\n",
        "\n",
        "The F1 score of the model on the training set is 0.92 and on the test set is 0.92. The F1 score is the harmonic mean of precision and recall. A high F1 score indicates a balance between precision and recall.\n",
        "\n",
        "The AUC (Area Under the Curve) of the model on the training set is 0.92 and on the test set is 0.92. AUC measures the performance of a binary classifier by plotting the true positive rate against the false positive rate. An AUC of 1 indicates a perfect classifier, while an AUC of 0.5 indicates a random classifier. In this case, the model has an AUC of 0.92 on both the training and test sets."
      ],
      "metadata": {
        "id": "luyD72U8ziVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing GridSearchCV for hyperparameter tuning in XGBoostClassifier**"
      ],
      "metadata": {
        "id": "AzzifaM7X2z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for XGBRegressor by gridsearchcv\n",
        "params={'n_estimators': [50,100,150],'max_depth': [3,5,9]}\n",
        "grid_xgboost_model=GridSearchCV(estimator=xgb,param_grid=params,cv=5,scoring='recall',verbose=5,n_jobs=-1)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the xgb_grid\n",
        "run_and_evaluate_model(grid_xgboost_model,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "hJsW2GXpm_FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Features Importance Of XG Boost Classifier**"
      ],
      "metadata": {
        "id": "nHVaNmCyYOtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing best estimator in varibale\n",
        "optimal__xg_model=grid_xgboost_model.best_estimator_"
      ],
      "metadata": {
        "id": "0hFrcJAhOFfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the feature importance of variable for XG Boost Classifier\n",
        "get_features_importance(optimal__xg_model,X_train)"
      ],
      "metadata": {
        "id": "OvyJYUfcORRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4"
      ],
      "metadata": {
        "id": "tQPfSnGQEK9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**LightGBM**"
      ],
      "metadata": {
        "id": "kF5YnpN7ELmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Instance of LightGBM\n",
        "lgbc=lgb.LGBMClassifier()"
      ],
      "metadata": {
        "id": "Nb_88VlyPSS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the lgbr_grid\n",
        "run_and_evaluate_model(lgbc,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "Lh8eq6FHPXFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "foEpv8RRZS7s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing GridSearchCV for hyperparameter tuning in LightGBM Classifier**"
      ],
      "metadata": {
        "id": "StmJJN8XaA_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for LightGBM by gridsearchcv\n",
        "lgbc_para={'n_estimators': [100,125,150],'max_depth': [7,10,15]}\n",
        "lgbc_grid=GridSearchCV(estimator=lgbc,param_grid=lgbc_para,cv=3,scoring='recall',verbose=5,n_jobs=-1)"
      ],
      "metadata": {
        "id": "96rHJ4C2Pgnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the lgbr_grid\n",
        "run_and_evaluate_model(lgbc_grid,X_train,X_test,y_train,y_test)"
      ],
      "metadata": {
        "id": "7XbdVbGyPlol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Features Importance Of LightGBM Classifier**"
      ],
      "metadata": {
        "id": "dd6oprWRacTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#storing best estimator in varibale\n",
        "optimal__rfc_model=lgbc_grid.best_estimator_"
      ],
      "metadata": {
        "id": "1CMJ28k3P4nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing the feature importance of variable for LightGBM \n",
        "get_features_importance(optimal__rfc_model,X_train)"
      ],
      "metadata": {
        "id": "UKV7owteP9T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Comparison Table**"
      ],
      "metadata": {
        "id": "StytEvynOxIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of dictionaries representing evaluation metrics for different models\n",
        "models_eval = [{'Model': 'LogisticRegression', 'Accuracy Train': 0.64, 'Accuracy Test': 0.64, 'Precision Train': 0.69,'Precision Test': 0.69,'Recall Train': 0.50,'Recall Test': 0.49,'F1 Train':0.58,'F1 Test': 0.58,'Auc Train': 0.64,'Auc Test':0.64},\n",
        "               {'Model': 'RandomForestClassifier', 'Accuracy Train': 0.97, 'Accuracy Test': 0.94, 'Precision Train': 0.97,'Precision Test': 0.92,'Recall Train': 0.98,'Recall Test': 0.95,'F1 Train':0.97,'F1 Test': 0.94,'Auc Train': 0.97,'Auc Test':0.94},\n",
        "               {'Model': 'XGBClassifier', 'Accuracy Train': 0.92, 'Accuracy Test': 0.92, 'Precision Train': 0.89,'Precision Test': 0.89,'Recall Train': 0.96,'Recall Test': 0.96,'F1 Train':0.92,'F1 Test': 0.92,'Auc Train': 0.92,'Auc Test':0.92},\n",
        "               {'Model': 'LightGBMClassifier', 'Accuracy Train': 0.95, 'Accuracy Test': 0.94, 'Precision Train': 0.95,'Precision Test': 0.93,'Recall Train': 0.96,'Recall Test': 0.94,'F1 Train':0.96,'F1 Test': 0.94,'Auc Train': 0.95,'Auc Test':0.94}]\n",
        "\n",
        "# function to create comparison table\n",
        "def create_comparison_table(models_eval):\n",
        "    comparison_table = pd.DataFrame(models_eval)\n",
        "    return comparison_table\n",
        "\n",
        "# create comparison table\n",
        "comparison_table = create_comparison_table(models_eval)\n",
        "comparison_table"
      ],
      "metadata": {
        "id": "NuyjtI2frgWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}