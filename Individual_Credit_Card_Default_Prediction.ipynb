{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rupalidawkoregithub/Credit-Card-Default-Prediction/blob/main/Individual_Credit_Card_Default_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Credit Card Default Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Rupali Dawkore\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem statement for credit card default prediction is to develop a model that accurately predicts the likelihood of a credit card holder defaulting on their debt payment in the near future. The model should take into account various factors such as the cardholder's credit history and other relevant financial information. The goal is to minimize false negatives (predicting a non-default when the cardholder actually defaults) and false positives (predicting a default when the cardholder is able to repay their debt) while maximizing the overall accuracy of the prediction."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing basic libraries for data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from datetime import datetime\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# Adding this to ignore future warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# importing missingo library which helps us to visualize the missing values\n",
        "import missingno as msno"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install --upgrade xlrd         ##run the cell at every restart of runtime"
      ],
      "metadata": {
        "id": "S4HZTCNMbKqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the Google Drive for Import the Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x6PuQc7VZXgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/Credit Card Default Prediction - Classification/default of credit card clients.xls',header=1)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked first five rows of dataset"
      ],
      "metadata": {
        "id": "MXK4Hkp9JKpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "_sRe1yLgpKOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checked last five rows of dataset"
      ],
      "metadata": {
        "id": "WhKmXv-3JWQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of rows and columns\n",
        "rows, columns = df.shape"
      ],
      "metadata": {
        "id": "Zw9fBat1D_LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of rows and columns\n",
        "print(\"Number of rows: \", rows)\n",
        "print(\"Number of columns: \", columns)"
      ],
      "metadata": {
        "id": "3k1RWu_hED2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 30000 rows and 25 columns in the dataset."
      ],
      "metadata": {
        "id": "HcFoK03V2R3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All types of columns dtypes are integer"
      ],
      "metadata": {
        "id": "JckJsvI2EOfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicated rows in default of credit card clients dataset: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not have any duplicated rows in the dataset and that is very good for us."
      ],
      "metadata": {
        "id": "5BV1D1R6GRN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values \n",
        "print(f\"Null values count in default of credit card clients dataset:\\n{df.isna().sum()}\\n\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Infinite values count in default of credit card clients dataset:\\n{df.isin([np.inf, -np.inf]).sum()}\\n\")"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't have null or infinite values default of credit card clients dataset."
      ],
      "metadata": {
        "id": "AH3sol0WG1ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values in default of credit card clients dataset\n",
        "msno.bar(df,figsize=(10,5), color=\"tab:green\")"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset dose not contains any NA values, null values and duplicates."
      ],
      "metadata": {
        "id": "h3Cp1u3N3FPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data provided is a sample of a credit card default dataset. The first row provides the header information, with each column indicating a feature of the credit card holder. The first column (ID) is the unique identifier for each record. The second column (LIMIT_BAL) indicates the credit limit of the credit card. The third column (SEX) indicates the gender of the cardholder. The fourth column (EDUCATION) indicates the level of education of the cardholder. The fifth column (MARRIAGE) indicates the marital status of the cardholder. The sixth column (AGE) indicates the age of the cardholder. The seventh to sixteenth columns (PAY_0 to PAY_9) indicate the repayment status for the last ten months. The remaining columns (BILL_AMT1 to BILL_AMT6, PAY_AMT1 to PAY_AMT6) indicate the amount of bill statement and amount paid in the last six months. The final column (default payment next month) is the target variable and indicates whether the cardholder defaulted on their payment in the next month (1) or not (0)."
      ],
      "metadata": {
        "id": "IFexwD-0IE_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Descriptive Statistics"
      ],
      "metadata": {
        "id": "S5PnxaCLMi9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we see the descriptive statistics such as count, mean, standard deviation, minimum,maximum, quantiles"
      ],
      "metadata": {
        "id": "HXcnnCaMp6ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe().T"
      ],
      "metadata": {
        "id": "0_yQ9-7-MIMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "- There are around 30000 distict credit card clients.\n",
        "- The average value of credit card Limits is Rs 1,67,484.\n",
        "- The Limited Balance has a high Standard deviation as the meadian value is Rs 1,40,000 and the extreme values as Rs 10,00,000.\n",
        "- Here the average is about 35 and meadian is 28 with a standard deviation of 9.2. This difference is explained by some very old people in the data set as given that the maximum age is 79.\n",
        "- Bill Amount and Pay Amount also shows us that there some people with extremely high bill amount which may be because for the higher Credit Limit or because of the pending dues added up. \n",
        "- Bill amount for all the months, the mean is around 40,000 to 50,000 with some extreme amount in bill amount 3 of Rs 16,64,089.\n",
        "- Pay amount for all the months, the mean is around Rs 4800 to Rs 5800, with some extreme values such as Rs 16,64,089.\n",
        "- As the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis)."
      ],
      "metadata": {
        "id": "QIUShNzQMzTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['LIMIT_BAL'].value_counts()"
      ],
      "metadata": {
        "id": "pxrH5tvc2Rr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ID:** ID of each client\n",
        "\n",
        "**LIMIT_BAL:** Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "\n",
        "**SEX:** Gender (1=male, 2=female)\n",
        "\n",
        "**EDUCATION:** (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "\n",
        "**MARRIAGE:** Marital status (1=married, 2=single, 3=others)\n",
        "\n",
        "**AGE:** Age in years\n",
        "\n",
        "**PAY_0:** Repayment status in September, 2005 (-1=pay duly, 1=payment delay for\n",
        "\n",
        "one month, 2=payment delay for two months,8=payment delay for eight months,\n",
        "\n",
        "9=payment delay for nine months and above)\n",
        "\n",
        "**PAY_2:** Repayment status in August, 2005 (scale same as above)\n",
        "\n",
        "**PAY_3:** Repayment status in July, 2005 (scale same as above)\n",
        "\n",
        "**PAY_4:** Repayment status in June, 2005 (scale same as above)\n",
        "\n",
        "**PAY_5:** Repayment status in May, 2005 (scale same as above)\n",
        "\n",
        "**PAY_6:** Repayment status in April, 2005 (scale same as above)\n",
        "\n",
        "**BILL_AMT1:** Amount of bill statement in September, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT2:** Amount of bill statement in August, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT3:** Amount of bill statement in July, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT4:** Amount of bill statement in June, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT5:** Amount of bill statement in May, 2005 (NT dollar)\n",
        "\n",
        "**BILL_AMT6:** Amount of bill statement in April, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT1:** Amount of previous payment in September, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT2:** Amount of previous payment in August, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT3:** Amount of previous payment in July, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT4:** Amount of previous payment in June, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT5:** Amount of previous payment in May, 2005 (NT dollar)\n",
        "\n",
        "**PAY_AMT6:** Amount of previous payment in April, 2005 (NT dollar)\n",
        "\n",
        "**default.payment.next.month:** Default payment (1=yes, 0=no)"
      ],
      "metadata": {
        "id": "IA2xmjFPC0-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "8EEhURuJOZCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "def get_all_unique_values(df):\n",
        "    for col in df.columns:\n",
        "        print(f\"Unique values in column '{col}':\")\n",
        "        print(df[col].unique())\n",
        "        print(\"-\"*50)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get and print all unique values\n",
        "get_all_unique_values(df)"
      ],
      "metadata": {
        "id": "dxNjz2e-OkI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We dont need to ID Column so we drop it\n",
        "df.drop(['ID'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "pHb1C_GNx5xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 25\n",
        "df.head(10)\n"
      ],
      "metadata": {
        "id": "KC3tdDdJfDc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copied dataset for data wrangling."
      ],
      "metadata": {
        "id": "9pwN0ofircYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "rN9dnBr4qYmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the columns didn’t make sense to me, so I decided to rename them into more understandable terms."
      ],
      "metadata": {
        "id": "HJscieUqPXqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename target column\n",
        "df1 = df.rename(columns={'default payment next month':'default'})"
      ],
      "metadata": {
        "id": "gJgm5bTyw4l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "df1.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "df1.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)"
      ],
      "metadata": {
        "id": "qT-NmuO_31fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#replacing values with there labels\n",
        "df1.replace({'SEX': {1 : 'Male', 2 : 'Female'}}, inplace=True)\n",
        "df1.replace({'EDUCATION' : {1 : 'Graduate School', 2 : 'University', 3 : 'High School', 4 : 'Others'}}, inplace=True)\n",
        "df1.replace({'MARRIAGE' : {1 : 'Married', 2 : 'Single', 3 : 'Others'}}, inplace = True)\n",
        "df1.replace({'default' : {1 : 'Yes', 0 : 'No'}}, inplace = True)"
      ],
      "metadata": {
        "id": "8kELTtNPqOnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After renamed look of dataset\n",
        "df1.head(10)"
      ],
      "metadata": {
        "id": "Eo6gnaWuczht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df1['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "2Z9nUqL3vTiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In education column, values such as 5,6 and 0 are unknown. Let's combine those values as others."
      ],
      "metadata": {
        "id": "vnfBwXBNvpk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replcae values with 5, 6 and 0 to Others\n",
        "df1.EDUCATION = df1.EDUCATION.replace({5: \"Others\", 6: \"Others\",0: \"Others\"})"
      ],
      "metadata": {
        "id": "hYi4t4jkvzGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking value count of education column after combine others values\n",
        "df1['EDUCATION'].value_counts()"
      ],
      "metadata": {
        "id": "_wkOtdzev6ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#category wise values\n",
        "df1['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "IzHawr0AzbZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In marriage column, 0 values are not known. Combine those values in others category."
      ],
      "metadata": {
        "id": "KeznT5KHzfFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#replace 0 with Others\n",
        "df1.MARRIAGE = df1.MARRIAGE.replace({0: \"Others\"})"
      ],
      "metadata": {
        "id": "5VgPP9eQzjz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking\n",
        "df1['MARRIAGE'].value_counts()"
      ],
      "metadata": {
        "id": "M2Fpju3jzx89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Distribution of default or non-default case**\n",
        "\n"
      ],
      "metadata": {
        "id": "vm1_Wc7CFOOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mi0 = df[df['default payment next month']==0]\n",
        "mi1 = df[df['default payment next month']==1]"
      ],
      "metadata": {
        "id": "v8gaQHwvDaR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi0.value_counts().sum()  # Non-Default => 0 or NO"
      ],
      "metadata": {
        "id": "5Pt0d8Tr6935"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mi1.value_counts().sum()  # Default => 1 or YES"
      ],
      "metadata": {
        "id": "nGWkw9g87TXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "Dbm8Ozgi5nnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_col=['LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_0', 'PAY_2',\n",
        "       'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2',\n",
        "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
        "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "# Checking Distribution of Non - Default \n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(con_col):\n",
        "    plt.subplot(6,5,num+1)\n",
        "    sns.distplot(mi0[i],color='g')\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-3dgG4bZEibT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Distribution of Default Case \n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(con_col):\n",
        "    plt.subplot(6,5,num+1)\n",
        "    sns.distplot(mi1[i],color='r')\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TMXxsGfR8rj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "wBU2X5xmQfma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. I have Changed the Name of a columns with its meaningfull name\n",
        "\n",
        "2. Grouped unknown EDUCATIONcategories (0,5,6) and re-assigned them to 4 (others)\n",
        "\n",
        "3. Grouped unknown MARRIAGEcategories (0) and re-assigned them to 3 (others)\n",
        "\n",
        "4. Checking Distribution of default or non-default case"
      ],
      "metadata": {
        "id": "a1mm8yqSQd__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate**"
      ],
      "metadata": {
        "id": "3ITwYZhn2xfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "RBp5zjWutAU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 1"
      ],
      "metadata": {
        "id": "irXXTAQ9XXtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting payment staus using countplot\n",
        "pay_col = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR']\n",
        "plt.figure(figsize=(20,20))\n",
        "for num,i in enumerate(pay_col):\n",
        "    plt.subplot(4,3,num+1)\n",
        "    sns.countplot(df1[i])\n",
        "    plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "47BbyFnw7x7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each column represents the status of payment for a given month:\n",
        "\n",
        "**1.'PAY_SEPT':** Represents the payment status in September.\n",
        "\n",
        "**2.'PAY_AUG':** Represents the payment status in August.\n",
        "\n",
        "**3.'PAY_JUL':** Represents the payment status in July.\n",
        "\n",
        "**4.'PAY_JUN':** Represents the payment status in June.\n",
        "\n",
        "**5.'PAY_MAY':** Represents the payment status in May.\n",
        "\n",
        "**6.'PAY_APR':** Represents the payment status in April.\n",
        "\n",
        "The countplot is used to visualize the distribution of the values in each of these columns. It shows the count of each unique value in the column, creating a histogram-like representation. The goal of this visualization is to see the frequency of each payment status, which can help understand the overall payment behavior of credit card customers in the data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cnpUeZe4SHqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 2"
      ],
      "metadata": {
        "id": "Ieq9ExSuXfIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of balance limit of credit card of customer**"
      ],
      "metadata": {
        "id": "5wpAXVI8TYl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "fig1 = px.histogram(df1, x = 'LIMIT_BAL', marginal = 'box',\n",
        "                    title = 'Distribution of balance limit of card', \n",
        "                    labels = {'x': 'Dollar($)', 'y': 'Number of card'},\n",
        "                   color_discrete_sequence=px.colors.qualitative.Antique)\n",
        "fig1.update_layout(width=900, height=700)\n",
        "fig1.show()"
      ],
      "metadata": {
        "id": "XETY9KJGOk2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is found that limit balance feature is right skewed, middle 50% of value lie between 50K to 240k.\n",
        "few of limit goes beyond 530k taiwan dollar"
      ],
      "metadata": {
        "id": "W7HxXThcTpd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# values count plot of Default\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'default', data = df1)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plot gives us an insight into the class distribution in the target variable. It is observed that the classes are not proportionate, indicating an imbalanced dataset. The data shows that there are **23,000 non-defaulters and 6,000 defaulters** , which means that this is a case of **imbalanced data**."
      ],
      "metadata": {
        "id": "4ToPhUdDwSvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 3"
      ],
      "metadata": {
        "id": "e03a4pAFXnHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# values count plot of Education\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'EDUCATION', data = df1)"
      ],
      "metadata": {
        "id": "QXduXaHax4Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# values count plot of Marriage\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot(x = 'MARRIAGE', data = df1)"
      ],
      "metadata": {
        "id": "C6I0OP9Gz-uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bivariate**"
      ],
      "metadata": {
        "id": "dg6v7vC4C7fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 4"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a qualification of the card holder**"
      ],
      "metadata": {
        "id": "dNIk-rF5cQ6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vis = df1['EDUCATION'].value_counts().reset_index()\n",
        "df_vis.columns = ['Education', 'No of people']\n",
        "fig1 = px.pie(df_vis, values = 'No of people', names = 'Education',color_discrete_sequence =  px.colors.sequential.Plasma,\n",
        "             title = 'Education qualification of credit card holder')\n",
        "fig1.update_layout(width=500, height=400)\n",
        "fig1.show()"
      ],
      "metadata": {
        "id": "acVZm9LrbEy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47% of them have university qualification\n",
        "35% of them have graduate school qualification 16% of them have high school qualification 2% of them qualification are unknown"
      ],
      "metadata": {
        "id": "tLtCDT1WcjO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 5"
      ],
      "metadata": {
        "id": "YjZFrlcaYHlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of Age of credit card holder**"
      ],
      "metadata": {
        "id": "VE7zhSO_ewU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig2 = px.histogram(df1, x = 'AGE', marginal = 'box',\n",
        "                    title = 'Distribution of Age of card holder', \n",
        "                    labels = {'x': 'Dollar($)', 'y': 'Noumber of card'},\n",
        "                   color_discrete_sequence=px.colors.qualitative.D3,\n",
        "                   nbins = 75)\n",
        "fig2.update_layout(width=500, height=400)\n",
        "fig2.update_traces(marker_line_width=1,marker_line_color=\"white\")"
      ],
      "metadata": {
        "id": "WU8qiyN9c5g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 6"
      ],
      "metadata": {
        "id": "wrgG6mz3YM0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Distribution of credit limit for default and non-default cases**"
      ],
      "metadata": {
        "id": "iYkzFXWPxE4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=\"default\", y=\"LIMIT_BAL\", data=df1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sAOmTqkfvsEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sex With Respective to Default** \n",
        "\n",
        "**or** \n",
        "\n",
        "**How many male and female are credit card defaulter.**"
      ],
      "metadata": {
        "id": "L4_gY-gnuWY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count plot for Sex and with respect to Default\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(10,5))\n",
        "sns.countplot(x = 'SEX', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'SEX', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "2CXJUzo6uB6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Education With Respective to Default**"
      ],
      "metadata": {
        "id": "4WwqcyhAy5ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count plot for EDUCATION and with respect to Default\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(18,5))\n",
        "sns.countplot(x = 'EDUCATION', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'EDUCATION', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "fyhz1Y_zxaYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Marriage With Respective to Default**"
      ],
      "metadata": {
        "id": "xxXhCvlF0XKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#count plot for MARRIAGE and with respect to IsDefaulter\n",
        "fig, axes = plt.subplots(ncols=2,figsize=(10,5))\n",
        "sns.countplot(x = 'MARRIAGE', ax = axes[0], data = df1)\n",
        "sns.countplot(x = 'MARRIAGE', hue = 'default',ax = axes[1], data = df1)"
      ],
      "metadata": {
        "id": "1IoQSuvB0V6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multivariate**"
      ],
      "metadata": {
        "id": "OlZxmcf4B0OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 7"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Creating new variables\n",
        "var = df1[['SEX', 'LIMIT_BAL','AGE']].copy()\n",
        "var['default'] = df1['default']\n",
        "\n",
        "#replace values in varibles with original names\n",
        "var.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}},inplace = True)\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking catplot for the given variable\n",
        "sns.catplot(x = \"SEX\",\n",
        "            y = \"LIMIT_BAL\",\n",
        "            kind = \"box\",\n",
        "            hue = \"default\",\n",
        "            color = '#0c4f4e',\n",
        "            data = var, saturation = 2,\n",
        "            margin_titles = True).set(title = \"limit balance by sex and default payments\");"
      ],
      "metadata": {
        "id": "EhQYGVFCZxx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Catplot is used in Seaborn to create categorical plots, which are plots that show the relationship between a categorical variable i.e SEX and one continuous variables i.e LIMIT_BAL. These plots are useful for visualizing the distribution and spread of data."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are more Female defaulters than men ,female have more ouliers in Limit Balance Variable"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 8 \n",
        "\n",
        "### Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "H5qsID3VgPxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize = [25, 15])\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chart - 9 \n",
        "### Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# sns.pairplot(df)\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pairplot was chosen in this case to visualize the relationships and distributions between multiple variables in the dataframe. Pairplots are a convenient way to quickly visualize relationships between variables."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pairplots can help identify relationships between variables, outliers, skewness, and other patterns in the data that can inform further analysis and modeling."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Null Hypothesis - There is no relation between Categorical Variables and Default\n",
        "\n",
        "   Alternate Hypothesis - There is a relationship between Categorical Variables and Default\n",
        "\n",
        "2. Null Hypothesis - There is no relation between Numeric Variable and Default\n",
        "\n",
        "   Alternate Hypothesis - There is a relation between Numeric Variable and Default"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1- Null Hypothesis - There is no relation between Categorical Variables and Default\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between Categorical Variables and Default"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "dQy5WiM2hTRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "def hypothesis_test_chi2(df, categorical_variable, alpha=0.05):\n",
        "    # Split the data into default and non-default groups\n",
        "    default = df[df['default payment next month'] == 1]\n",
        "    non_default = df[df['default payment next month'] == 0]\n",
        "\n",
        "    # Conduct a chi-square test for the independence of the categorical variable and default\n",
        "    cont = pd.crosstab(df['default payment next month'], df[categorical_variable])\n",
        "    chi2, p_value, dof, expected = chi2_contingency(cont)\n",
        "\n",
        "    # Make a decision based on the p-value and alpha\n",
        "    if p_value < alpha:\n",
        "        return f\"Reject the null hypothesis. There is a significant association between {categorical_variable} and default.\"\n",
        "    else:\n",
        "        return f\"Fail to reject the null hypothesis. There is no significant association between {categorical_variable} and default.\""
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of categorical variables\n",
        "categorical = ['SEX', 'EDUCATION', 'MARRIAGE','PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n",
        "# Loop through the list of categorical variables\n",
        "for categorical_variable in categorical:\n",
        "    result = hypothesis_test_chi2(df, categorical_variable)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "6yMoSKluczCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function hypothesis_test_chi2 performs a chi-square test for independence to obtain the p-value. The chi2_contingency function from the scipy.stats library is used to calculate the chi-square statistic and the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chi-square test for independence is a common test for determining if there is a significant association between two categorical variables. In this case, the categorical variable of interest and the binary outcome variable \"default\" are being tested for independence. The choice of the chi-square test for independence is appropriate for this type of analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Null Hypothesis - There is no relation between Numeric Variable and Default\n",
        "\n",
        "Alternate Hypothesis - There is a relation between Numeric Variable and Default"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "def hypothesis_test_t(df, numerical_variable, alpha=0.05):\n",
        "    # Split the data into default and non-default groups\n",
        "    default = df[df['default payment next month'] == 1][numerical_variable]\n",
        "    non_default = df[df['default payment next month'] == 0][numerical_variable]\n",
        "\n",
        "    # Conduct a two-sample t-test for the means of the numerical variable for default and non-default groups\n",
        "    t, p_value = ttest_ind(default, non_default)\n",
        "\n",
        "    # Make a decision based on the p-value and alpha\n",
        "    if p_value < alpha:\n",
        "        return f\"Reject the null hypothesis. There is a significant difference in the means of {numerical_variable} between default and non-default groups.\"\n",
        "    else:\n",
        "        return f\"Fail to reject the null hypothesis. There is no significant difference in the means of {numerical_variable} between default and non-default groups.\""
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['LIMIT_BAL','AGE','BILL_AMT1', 'BILL_AMT2',\n",
        "       'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n",
        "       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
        "for col in numerical_columns:\n",
        "    result = hypothesis_test_t(df, col, alpha=0.05)\n",
        "    print(result)"
      ],
      "metadata": {
        "id": "usSw4V8ddQ4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function hypothesis_test_t performs a two-sample t-test to obtain the p-value. The ttest_ind function from the scipy.stats library is used to calculate the t-statistic and the p-value"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test is a common test for determining if there is a significant difference in means between two groups. In this case, the two groups are the default and non-default groups for a numerical variable. The choice of the two-sample t-test is appropriate for this type of analysis when the numerical variable is continuous and the sample size is relatively small."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To identify the categorical, numerical columns, and input and target columns"
      ],
      "metadata": {
        "id": "Ykqtm8fJoUb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "q65VtquKusBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# independant variable\n",
        "Input_columns=[ 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'PAY_SEPT',\n",
        "       'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR', 'BILL_AMT_SEPT',\n",
        "       'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "       'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL',\n",
        "       'PAY_AMT_JUN', 'PAY_AMT_MAY', 'PAY_AMT_APR']\n",
        "# dependent variable\n",
        "Target_column=[\"default\"]"
      ],
      "metadata": {
        "id": "E8hv2sVflkVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = ['SEX','EDUCATION','MARRIAGE','PAY_SEPT', 'PAY_AUG',\n",
        "       'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']"
      ],
      "metadata": {
        "id": "QcG-5RMWv0B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['LIMIT_BAL','AGE','BILL_AMT_SEPT',\n",
        "       'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "       'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL',\n",
        "       'PAY_AMT_JUN', 'PAY_AMT_MAY', 'PAY_AMT_APR']"
      ],
      "metadata": {
        "id": "5reteQ9fCCOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "rQp32GBNkf9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking for null value by using .info\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "y2kP_7mfkVDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking skewness for numerical columns\n",
        "import scipy.stats as stats\n",
        "for col in numerical_columns:\n",
        "    skewness = stats.skew(df1[col])\n",
        "    print(\"Skewness of column {}: {:.2f}\".format(col, skewness))"
      ],
      "metadata": {
        "id": "CRMmstBAGX-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for num,cols in enumerate(numerical_columns):\n",
        "    plt.subplot(5,3,num+1)\n",
        "    sns.boxplot(df1[cols])\n",
        "    plt.title(f'{cols.title()}',weight='bold')\n",
        "    plt.tight_layout()\n",
        "    #print(' Box Plot of',cols)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UuCI52wyDRo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the boxplot, it can be observed that the columns: LIMIT_BAL , AGE ,  BILL_AMT1 , BILL_AMT2 , BILL_AMT3 , BILL_AMT4 , BILL_AMT5 , BILL_AMT6 , PAY_AMT1 , PAY_AMT2 , PAY_AMT3 , PAY_AMT4 , 'PAY_AMT5 , and PAY_AMT6 contain outliers."
      ],
      "metadata": {
        "id": "AdoZ4EyGFB7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of Outlier Records:\")\n",
        "\n",
        "for col in numerical_columns:\n",
        "    upper = df1[col].quantile(0.75) + 1.5 * (df1[col].quantile(0.75) - df1[col].quantile(0.25))\n",
        "    outliers = df1[df1[col] > upper][col].count()\n",
        "    print(\"{}: {}\".format(col, outliers))\n"
      ],
      "metadata": {
        "id": "SpMaCYfZIYlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculates the upper bound of outliers using the interquartile range (IQR) and then counts the number of values in each column that are greater than this upper bound."
      ],
      "metadata": {
        "id": "FV5PCr_HKa11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capping"
      ],
      "metadata": {
        "id": "CVH9KsLiKLjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_columns:\n",
        "    upper = df1[col].quantile(0.75) + 1.5 * (df1[col].quantile(0.75) - df1[col].quantile(0.25))\n",
        "    df1[col] = np.where(df1[col] > upper, upper, df1[col])\n",
        "    print(\"{}: {}\".format(col, outliers))\n"
      ],
      "metadata": {
        "id": "0tBnGRp7Jf5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The upper bound of outliers is calculated using the interquartile range (IQR) as in the previous code. Then, using np.where, the values in each column that are greater than the upper bound are replaced with the upper bound. And performs the capping of the outliers."
      ],
      "metadata": {
        "id": "pBxGs61fKjYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rechecking outliers for numerical columns\n",
        "plt.figure(figsize=(15,15))\n",
        "for num,cols in enumerate(numerical_columns):\n",
        "    plt.subplot(5,3,num+1)\n",
        "    sns.boxplot(df1[cols])\n",
        "    plt.title(f'{cols.title()}',weight='bold')\n",
        "    plt.tight_layout()\n",
        "    #print(' Box Plot of',cols)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jeeqnOWaKBeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "3O8m71yuiuB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "def replace_values(df1, col, values):\n",
        "    # This code is replacing all values of -2, -1, and 0 in the specified columns with 0.\n",
        "    fil = (df1[col] == -2) | (df1[col] == -1) | (df1[col] == 0)\n",
        "    df1.loc[fil, col] = values\n",
        "\n",
        "columns = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\t\t\t\t\n",
        "values = 0\n",
        "\n",
        "for col in columns:\n",
        "    replace_values(df1, col, values)"
      ],
      "metadata": {
        "id": "eb54Qr7qzotf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is replacing all values of -2, -1, and 0 in the specified columns with 0."
      ],
      "metadata": {
        "id": "bUCFBPhunRmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_values(df1, col, values):\n",
        "    #  This code is replacing all values greater then 0 in the specified columns with 0.\n",
        "    fil = (df1[col] < 0)\n",
        "    df1.loc[fil, col] = values\n",
        "\n",
        "columns = ['BILL_AMT_SEPT', 'BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY', 'BILL_AMT_APR']\t\t\t\t\t\t\n",
        "values = 0\n",
        "\n",
        "for col in columns:\n",
        "    replace_values(df1, col, values)"
      ],
      "metadata": {
        "id": "DIC53I2czu33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As negative Bill Amount paid indicates that the person has paid his due payment already. Hence, we transform the data as above"
      ],
      "metadata": {
        "id": "M9DuJZL3nvOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "fil = (df1.PAY_SEPT == 0) & (df1.PAY_AUG == 0) & (df1.PAY_JUL == 0) & (df1.PAY_JUN == 0) & (df1.PAY_MAY == 0) & (df1.PAY_APR == 0) & (df1['default'] == 1)\n",
        "df1.loc[fil,'default'] = 0\n"
      ],
      "metadata": {
        "id": "Tv9i8Ke9zyzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As customers who have not defaulted in any month cannot be potential defaulters hence, we have transformed the data as above."
      ],
      "metadata": {
        "id": "ngs9P5sBoPGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "fil = (df1.PAY_SEPT > 0) & (df1.PAY_AUG > 0) & (df1.PAY_JUL > 0) & (df1.PAY_JUN > 0) & (df1.PAY_MAY > 0) & (df1.PAY_APR > 0) & (df1['default'] == 0)\n",
        "df1.loc[fil,'default'] = 1"
      ],
      "metadata": {
        "id": "b83Af-zqz2wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As customers who have defaulted in every month are the potential defaulters hence, we have transformed the data as above."
      ],
      "metadata": {
        "id": "SIW4N34WoZiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binning the 'AGE' column**"
      ],
      "metadata": {
        "id": "KgEt62cfwDOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1['AGE'].min())\n",
        "print(df1['AGE'].max())"
      ],
      "metadata": {
        "id": "685bpuFjuoWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see here min age is 21.0 and maximum age is 60.5 in our dataset"
      ],
      "metadata": {
        "id": "t2Szng5vr315"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating function to create the cohort for age group\n",
        "def age(x):\n",
        "    if x in range(21,41):\n",
        "        return 1\n",
        "    elif x in range(41,61):\n",
        "        return 2\n",
        "    elif x in range(61,80):\n",
        "        return 3\n",
        "\n",
        "df1['AGE']=df1['AGE'].apply(age)"
      ],
      "metadata": {
        "id": "6bx2d3HevP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the ages are into three categories, By applying the categorize_age function to the AGE column of the dataframe, the original ages in the column are transformed into categorical values."
      ],
      "metadata": {
        "id": "6olGOX3xsmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#visualizing age group\n",
        "plt.figure(figsize=(10,8),dpi=60)\n",
        "sns.countplot(x=df1['AGE'].sort_values(),data=df1,hue='default')"
      ],
      "metadata": {
        "id": "cVlKqw3Ys3fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "r9WyYbtJ0A4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**obsevation:**\n",
        "In aur dataset we can clearly see that most of the credit card holder are of age between 21 to 41 , so we can say that company's target customer are mostly youngster."
      ],
      "metadata": {
        "id": "A2SBiAIFtEjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binning the 'PAY' column**"
      ],
      "metadata": {
        "id": "nX_f50l4lSlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bins(x):\n",
        "    if x == -2:\n",
        "        return 'Paid Duly'\n",
        "    if x == 0:\n",
        "        return 'Paid Duly'\n",
        "    if x == -1:\n",
        "        return 'Paid Duly'\n",
        "    if x in range(1,4):\n",
        "        return '1 to 3'\n",
        "    if x in range(4,7):\n",
        "        return '4 to 6'\n",
        "    if x in range(7,9):\n",
        "        return '7 to 9'\n",
        "\n",
        "for i in df1[['PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']]:\t\t\t\t\t\t\n",
        "    df1[i]=df1[i].apply(bins)"
      ],
      "metadata": {
        "id": "BuAUa4jWjYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values of the columns 'PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', and 'PAY_APR' in the DataFrame 'df_copy'.\n",
        "\n",
        "For each value of these columns, the 'bins' function is being applied, which maps the values to one of four categorical bins:\n",
        "\n",
        "Paid Duly (for values of -2, 0, or -1)\n",
        "\n",
        "1 to 3 (for values in the range 1 to 3)\n",
        "\n",
        "4 to 6 (for values in the range 4 to 6)\n",
        "\n",
        "7 to 9 (for values in the range 7 to 9)"
      ],
      "metadata": {
        "id": "E7Qu8Ux6ssjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "U_KGE_hdje_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encoding**"
      ],
      "metadata": {
        "id": "eDBVI_xvWGF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "id": "VScLF5iqWPjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#label encoding\n",
        "encoders_nums = {\"SEX\":{\"Male\":0,\"Female\":1}, \"default\":{\"Yes\":1, \"No\":0}}\n",
        "df1 = df1.replace(encoders_nums)"
      ],
      "metadata": {
        "id": "LDz2gJVuWEGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for changed labels\n",
        "df1.head(5)"
      ],
      "metadata": {
        "id": "_-PBJj01WtBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "8fr9JHJXW4po"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Importing\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#categorical features\n",
        "categorical_cols_to_encode = ['EDUCATION', 'MARRIAGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR']"
      ],
      "metadata": {
        "id": "UUMrxpSUUxA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(df1[categorical_cols_to_encode])\n",
        "encoded_cols = list(encoder.get_feature_names(categorical_cols_to_encode))\n",
        "df1[encoded_cols] = encoder.transform(df1[categorical_cols_to_encode])"
      ],
      "metadata": {
        "id": "VyALwAq3U0Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_cols"
      ],
      "metadata": {
        "id": "LmrnFav7U3Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.drop(['EDUCATION', 'MARRIAGE','PAY_SEPT','PAY_AUG','PAY_JUL','PAY_JUN','PAY_MAY','PAY_APR'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Jry5caNMU6-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "NhKaCBcmU-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = 47\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "Ky4dZJ6zk0RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.columns"
      ],
      "metadata": {
        "id": "uS5AyIxV-yaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Checking if Data is Imbalance"
      ],
      "metadata": {
        "id": "YyAiJYzUwYMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((df1['default'].value_counts()/df1['default'].shape)*100)\n",
        "sns.countplot(df1['default'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIv2vxXKwe5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.default.value_counts() "
      ],
      "metadata": {
        "id": "7_bJM98Vz3-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Yes, Here we can see that the data is imbalanced. bcoz the Based on the values 77.88% for 0 and 22.12% for 1), it appears that the dataset is imbalanced. An imbalanced dataset refers to a situation where the distribution of classes (in this case, 0s and 1s) is not equal. In the given code, the class distribution is heavily skewed towards class 0, with 77.88% of the observations being class 0 and only 22.12% being class 1. This imbalance can lead to difficulties in training machine learning models, as the model may be biased towards predicting the majority class. As a result, the model's performance on the minority class may be poor."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
        "print('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with the mean of the column\n",
        "# df1.fillna(df1.mean(), inplace=True)\n",
        "\n",
        "# #importing SMote to make our dataset balanced\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# # Apply SMOTE to balance the dataset\n",
        "# smote = SMOTE()\n",
        "# X_smote, y_smote = smote.fit_resample(df1.drop('default', axis=1), df1['default'])\n",
        "\n",
        "# print('Original dataset shape', len(df1))\n",
        "# print('Resampled dataset shape', len(y_smote))"
      ],
      "metadata": {
        "id": "3u8Hk4aezGIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing SMOTE to handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE()\n",
        "\n",
        "# fit predictor and target variable\n",
        "x_smote, y_smote = smote.fit_resample(df1[(i for i in list(df1.describe(include='all').columns) if i != 'default')], df1['default'])\n",
        "\n",
        "print('Original unbalanced dataset shape', len(df1))\n",
        "print('Resampled balanced dataset shape', len(y_smote))"
      ],
      "metadata": {
        "id": "v2kI1n6m32Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new dataframe from balanced dataset after SMOTE\n",
        "balanced_df = pd.DataFrame(x_smote, columns=list(i for i in list(df1.describe(include='all').columns) if i != 'default'))"
      ],
      "metadata": {
        "id": "BTtEZsjr49A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding target variable to new created dataframe\n",
        "balanced_df['default'] = y_smote\n"
      ],
      "metadata": {
        "id": "JMiJf0J-0Jpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for class imbalance\n",
        "plt.figure(figsize=(5,5))\n",
        "sns.countplot('default', data = balanced_df)\n"
      ],
      "metadata": {
        "id": "-nQidmkryvfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of balanced dataframe\n",
        "balanced_df.shape"
      ],
      "metadata": {
        "id": "098NIxs1JNU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "ma2oOSCzQcf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #seperating dependant and independant variabales\n",
        "# X = balanced_df[(list(i for i in list(balanced_df.describe(include='all').columns) if i != 'dfault'))]\n",
        "# y = balanced_df['default']"
      ],
      "metadata": {
        "id": "1bJ2CYFKRUNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X.shape"
      ],
      "metadata": {
        "id": "-4g8jhTnReRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y.shape"
      ],
      "metadata": {
        "id": "PBJWMAf-RisG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique used to handle the imbalanced dataset is Synthetic Minority Over-sampling Technique (**SMOTE**). SMOTE is a commonly used oversampling technique for imbalanced datasets, which creates synthetic samples of the minority class instead of simply duplicating existing samples. This helps to balance the class distribution and reduce overfitting, which can occur when a model is trained on a highly imbalanced dataset. The reason for choosing SMOTE in this case is because it is effective in handling class imbalance by generating new samples of the minority class, while still preserving the characteristics of the original data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# independent variable (estimator)\n",
        "X = balanced_df.drop(\"default\", axis = 1)\n",
        "\n",
        "# dependent variable (label)\n",
        "y = balanced_df[\"default\"]"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test\n",
        "#importing libraries for splitting data into training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle = True, random_state = 11)"
      ],
      "metadata": {
        "id": "vXWCRT4XeV_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "sZwP335Etgq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "sx2OgJYYSbQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler "
      ],
      "metadata": {
        "id": "j3zvjbyffsV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X_train_scale = scaler.fit_transform(X_train)\n",
        "X_test_scale = scaler.transform(X_test)\n",
        "\n",
        "X_test = pd.DataFrame(X_test_scale, columns = X_test.columns)\n",
        "X_train = pd.DataFrame(X_train_scale, columns = X_train.columns)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, plot_precision_recall_curve\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "f7k2UwD08gUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Creating Function**"
      ],
      "metadata": {
        "id": "ixuWvBE97yGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_score(model, X_train, y_train, X_test, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "\n",
        "    train_accuracy = round(accuracy_score(y_train_pred, y_train), 3)\n",
        "    accuracy = round(accuracy_score(y_pred, y_test), 3)\n",
        "    precision = round(precision_score(y_pred, y_test), 3)\n",
        "    recall = round(recall_score(y_pred, y_test), 3)\n",
        "    f1 = round(f1_score(y_pred, y_test), 3)\n",
        "    roc_score = round(roc_auc_score(y_pred, y_test), 3)\n",
        "\n",
        "    print(\"The accuracy on train data is \", train_accuracy)\n",
        "    print(\"The accuracy on test data is \", accuracy)\n",
        "    print(\"The precision on test data is \", precision)\n",
        "    print(\"The recall on test data is \", recall)\n",
        "    print(\"The f1 on test data is \", f1)\n",
        "    print(\"The roc_score on test data is \", roc_score)\n"
      ],
      "metadata": {
        "id": "TRMwXJJwl1L0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating function to get features importance of all the tree based model\n",
        "def get_features_importance(optimal_model,X_train):\n",
        "  imp_feat=pd.DataFrame(index=X.columns,data=optimal_model.feature_importances_,columns=['importance'])\n",
        "  imp_feat=imp_feat[imp_feat['importance']>0]\n",
        "  imp_feat=imp_feat.sort_values('importance')\n",
        "  plt.figure(figsize=(15,5))\n",
        "  print(f'==========================Features Importance============================\\n\\n {optimal_model}\\\n",
        "  \\n=========================================================================\\n') \n",
        "  sns.barplot(data=imp_feat,x=imp_feat.index,y='importance')\n",
        "  plt.xticks(rotation=90);\n"
      ],
      "metadata": {
        "id": "o38-_bAG3JXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression**"
      ],
      "metadata": {
        "id": "wff_dQO1_Msv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating Instance of Logistic Regression\n",
        "LR= LogisticRegression()\n",
        "model_score(LR, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "eGbEWgLzNVjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implementing GridSearch for Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "lPS95tPMrcIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a set of hyperparameters to search over\n",
        "param_grid = {'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "              'C': [0.1, 1, 10, 100],\n",
        "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "              'fit_intercept': [True, False],\n",
        "              'max_iter': [100, 500, 1000]}\n",
        "\n",
        "# Create a GridSearchCV object and fit it to the data\n",
        "grid = GridSearchCV(LR, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and the best score\n",
        "print(\"Best hyperparameters: \", grid.best_params_)\n",
        "print(\"Best accuracy: \", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance of LogisticRegression with the best hyperparameters\n",
        "LR_best = LogisticRegression(C=1, fit_intercept=True, max_iter=100, penalty='l2', solver='lbfgs')\n",
        "\n",
        "# Fit the model to the training data\n",
        "LR_best.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importance from the fitted model\n",
        "importance = LR_best.coef_[0]\n",
        "\n",
        "# Create a DataFrame to store the feature names and importance\n",
        "imp_feat = pd.DataFrame({'Features': X_train.columns, 'Importance': importance})\n",
        "imp_feat = imp_feat.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot the feature importance\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.barplot(x='Features', y='Importance', data=imp_feat)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "myzTI45Z8Bxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Rf = RandomForestClassifier(n_estimators=50)\n",
        "model_score(Rf, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "hZaNWNTZEFly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**implementing gridsearch for hyperparameter tuning in Random Forest**"
      ],
      "metadata": {
        "id": "5uGwoy4Ft-xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# finding the best parameters for rfc_model by gridsearchcv\n",
        "grid_values = {'n_estimators': [100,125,150],'max_depth': [7,10,15],'criterion': ['entropy']}\n",
        "grid_rfc_model = GridSearchCV(estimator=Rf,param_grid = grid_values, scoring='balanced_accuracy',cv=3,verbose=5,n_jobs=-1)"
      ],
      "metadata": {
        "id": "N6enFEb1sodD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training and evaluating the Random forest with hyperparameter tuing\n",
        "model_score(grid_rfc_model,X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "guzP2q8WssMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting data into XG Boosting Classifier\n",
        "xgb = XGBClassifier()\n",
        "model_score(xgb, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "07mUsBSLjjhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}